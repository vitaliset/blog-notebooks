{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boruta Feature Selection\n",
    "\n",
    "Ter menos atributos nos retorna um modelo menos complexo. Para ilustrar essa afirmação, temos que a [dimensão-VC](https://youtu.be/Dc0sr0kdBVI) (medida de complexidade de uma família de hipóteses) de um perceptron (classificador linear) é $d+1$ em que $d$ é a número de variáveis. Na prática, isso significa, menos chance de overfitting e, consequentemente, nos dá modelos mais estáveis e com melhor performance fora do laboratório. Por isso, na minha opinião, a seleção de variáveis é uma das mais poderosas ferramentas data-centric.\n",
    "\n",
    "Entretanto, esse assunto não é visto com o cuidado devido na maioria dos cursos de Aprendizado de Máquina. São apresentados poucos métodos e de maneira superficial. Focando ainda em técnicas que não são escaláveis com o aumento de variáveis e por isso são impraticáveis (como as estratégias gulosas de [`sklearn.feature_selection.SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)).\n",
    "\n",
    "No DataLab, seleção de variáveis se torna extremamente relevante pela natureza dos problemas que trabalhamos. Na grande maioria dos casos temos algumas milhares de variáveis do Bureau da Serasa e não é fácil identificar de antemão quais serão as que nos darão mais ganhos. É necessário aplicar técnicas que são robustas à grandeza de variáveis que temos ao mesmo tempo que garantimos uma seleção que faça sentido.\n",
    "\n",
    "Neste post iremos motivar a construção do Boruta, uma das técnicas mais utilizadas pelos cientistas do DataLab na seleção de features, com algumas dicas de uso prático. Ilustraremos ainda o uso da função [`boruta.BorutaPy`](https://github.com/scikit-learn-contrib/boruta_py), do ambiente [scikit-learn-contrib](https://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md) (ou seja, compátivel com qualquer biblioteca que siga o [padrão de código do scikit-learn](https://scikit-learn.org/stable/developers/develop.html).\n",
    "\n",
    "___\n",
    "\n",
    "Para ilustrar o problema de seleção de features, utilizaremos o [`sklearn.datasets.make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) para criar um problema genérico de classificação em que podemos definir, como um parâmetro da função, o número de variáveis úteis para o problema de previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST PARA O MEU BLOG: ESTRATÉGIAS DE FEATURE SELECTION DO KSLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>...</th>\n",
       "      <th>column_17</th>\n",
       "      <th>column_18</th>\n",
       "      <th>column_19</th>\n",
       "      <th>column_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.050478</td>\n",
       "      <td>-1.323568</td>\n",
       "      <td>0.912474</td>\n",
       "      <td>1.009796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800511</td>\n",
       "      <td>1.238946</td>\n",
       "      <td>0.209659</td>\n",
       "      <td>-0.491636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.580834</td>\n",
       "      <td>-2.747104</td>\n",
       "      <td>1.777419</td>\n",
       "      <td>1.850430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524088</td>\n",
       "      <td>0.152355</td>\n",
       "      <td>-0.822420</td>\n",
       "      <td>1.121031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.885704</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>0.631813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262561</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>-0.137372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.525438</td>\n",
       "      <td>-2.967793</td>\n",
       "      <td>1.884777</td>\n",
       "      <td>1.924410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617652</td>\n",
       "      <td>-0.316073</td>\n",
       "      <td>0.615771</td>\n",
       "      <td>1.203884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.076826</td>\n",
       "      <td>-1.014619</td>\n",
       "      <td>0.752233</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326745</td>\n",
       "      <td>0.300474</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>-1.138833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   column_1  column_2  column_3  column_4  ...  column_17  column_18  \\\n",
       "0 -1.050478 -1.323568  0.912474  1.009796  ...   1.800511   1.238946   \n",
       "1 -1.580834 -2.747104  1.777419  1.850430  ...  -0.524088   0.152355   \n",
       "2 -0.885704 -0.614600  0.501004  0.631813  ...   0.262561   0.193590   \n",
       "3 -1.525438 -2.967793  1.884777  1.924410  ...  -0.617652  -0.316073   \n",
       "4 -1.076826 -1.014619  0.752233  0.885267  ...   0.326745   0.300474   \n",
       "\n",
       "   column_19  column_20  \n",
       "0   0.209659  -0.491636  \n",
       "1  -0.822420   1.121031  \n",
       "2   0.850898  -0.137372  \n",
       "3   0.615771   1.203884  \n",
       "4   0.622207  -1.138833  \n",
       "\n",
       "[5 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "N_FEATURES = 20\n",
    "\n",
    "X, y = \\\n",
    "make_classification(n_samples=1000,\n",
    "                    n_features=N_FEATURES,\n",
    "                    n_informative=2,\n",
    "                    n_redundant=2,\n",
    "                    n_classes=2,\n",
    "                    flip_y=0.1,\n",
    "                    shuffle=False,\n",
    "                    random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f'column_{i+1}' for i in range(N_FEATURES)])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train...\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos escolhendo 2 features informativas e 2 features redundantes, temos que as 4 features mais importantes são as colunas: column_1, column_2, column_3 e column_4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção do Boruta\n",
    "\n",
    "## Medindo a importância de uma variável\n",
    "\n",
    "Uma das técnicas mais comuns para selecionar as variáveis é aproveitar-se de modelos que de alguma forma selecionam as variáveis no processo de treinamento. Árvores e, consequentemente, cômites de árvores são talvez o melhor exemplo disso: pela estratégia gulosa de fazer sempre a melhor quebra possível (de acordo com algum critério de melhor, usualmente relacionada a pureza das folhas criadas), estamos sempre escolhendo a melhor quebra da melhor variável para ser feita naquela etapa. Variáveis pouco discriminativas são escolhidas muito menos que as variáveis que de fato ajudam a fazer a previsão.\n",
    "\n",
    "Esse processo naturalmente deriva medidas de importâncias para as variáveis como o número de vezes que ela é utilizada (esse é o modo default do atributo `feature_importance_` dos ensembles do LGBM, como o [`lightgbm.LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) e o análogo para regressão) ou uma ponderação do ganho no critério de escolha das features (essa é a forma default dos ensembles de árvores do sklearn, como o [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), o [`sklearn.ensemble.ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html), e o [`sklearn.ensemble.HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) e também vira o atributo do LGBM quando setamos o `importance_type='gain'`).\n",
    "\n",
    "Com essa medida natural de importância podemos é razoável ordenar nossas variáveis da mais importante para a menos importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.278748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.201150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.092612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.018641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_18</td>\n",
       "      <td>0.017565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.016912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  feature_importance\n",
       "0      column_2            0.278748\n",
       "1      column_3            0.201150\n",
       "2      column_4            0.092612\n",
       "..          ...                 ...\n",
       "17    column_16            0.018641\n",
       "18    column_18            0.017565\n",
       "19    column_20            0.016912\n",
       "\n",
       "[20 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42).fit(X, y)\n",
    "\n",
    "df_imp = \\\n",
    "(pd.DataFrame(list(zip(X.columns, rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem algumas outras formas de metrificar a importância de uma variável, como por exemplo utilizando suas contribuições de [valores SHAP](https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30). Como o [`shap.Explainer(model)(X)`](https://github.com/slundberg/shap) nos retorna uma medida de quanto aquela variável contribuiu na nossa previsão, pegar a sua média entre todos os exemplos nos dá uma forma de ver o quão útil ela foi para discriminar os exemplos como um todo. Para os valores não se cancelarem (imagine uma variável que para determinados valores joga a previsão para cim e em outros valores joga a previsão para baixo), tomamos o módulo antes de fazer a média."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>shap_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.197645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.107211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.043797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_5</td>\n",
       "      <td>0.005099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  shap_importance\n",
       "0      column_2         0.197645\n",
       "1      column_3         0.107211\n",
       "2      column_4         0.043797\n",
       "..          ...              ...\n",
       "17    column_16         0.005268\n",
       "18     column_5         0.005099\n",
       "19    column_20         0.005019\n",
       "\n",
       "[20 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(rfc)\n",
    "shap_vals = explainer.shap_values(X)\n",
    "\n",
    "df_imp_shap = \\\n",
    "(pd.DataFrame(list(zip(X.columns, np.abs(shap_vals[0]).mean(axis=0))),\n",
    "              columns=['feature_name', 'shap_importance'])\n",
    " .sort_values(by='shap_importance', ascending=False)\n",
    " .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_imp_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare apenas que a ordem das importâncias dada pelo SHAP é diferente da ordem de importâncias dada pelo atributo de `feature_importance_` usual do estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['2', '3', '4', '1', '10', '6', '14', '11', '9', '15', '12', '7',\n",
       "        '8', '13', '19', '17', '5', '16', '18', '20'], dtype=object),\n",
       " array(['2', '3', '4', '1', '9', '10', '6', '7', '8', '11', '19', '12',\n",
       "        '14', '13', '17', '15', '18', '16', '5', '20'], dtype=object))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_imp.feature_name.str.split('_').str[1].values, df_imp_shap.feature_name.str.split('_').str[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_imp.feature_name.str.split('_').str[1].values,\n",
    "                    df_imp_shap.feature_name.str.split('_').str[1].values\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_diffs = sum(x != y for x, y in zip(df_imp_shap.feature_name, df_imp.feature_name))\n",
    "n_diffs // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não falamos do Boruta, mas ele se utiliza dessa ordenação para fazer suas análises e é implementado, usualmente utilizando medida de importância do estimador. Essa diferença motivou alguns contribuidores a implementar o Boruta-Shap. Infelizmente a biblioteca não é tão bem estruturada quanto a do Boruta, mas pode ser uma opção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionando as `K` melhores\n",
    "\n",
    "Se queremos que nosso modelo tenha apenas as `K` features mais úteis, a maneira natural de escolher elas seria pegar as `K` variáveis com maiores valores de importância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column_2', 'column_3', 'column_4', 'column_1']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 4\n",
    "\n",
    "(df_imp\n",
    " .head(K)\n",
    " .feature_name\n",
    " .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma das estratégias mais comuns de se fazer seleção de features no mercado, mas levanta algumas questões. A primeira e mais imediata é: como escolher o número de variáveis `K` ideal. Nesse caso ilustrativo, sabemos que 4 variáveis é o número ideal, mas na maioria dos casos aplicação real é irrealista ter esse número de antemão.\n",
    "\n",
    "Uma estratégia muito utilizada, mas que não vamos focar muito, é aumentar a lista de features do modelo seguindo a ordenação dada pelo modelo treinado em todas as features. Encarando esse valor `K` como um hiper-parâmetro que estamos otimizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class SelectKTop(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"pass\"\"\"\n",
    "\n",
    "    def __init__(self, K=5, random_state=42):\n",
    "        self.K = K\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = self._validate_data(\n",
    "            X,\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "        \n",
    "        self._rfc_ = RandomForestClassifier(random_state=self.random_state).fit(X, y)\n",
    "        \n",
    "        self._selected_index_list_ = \\\n",
    "        (pd.DataFrame(list(zip(range(self.n_features_in_), self._rfc_.feature_importances_)),\n",
    "                      columns=['feature_name', 'feature_importance'])\n",
    "         .sort_values(by='feature_importance', ascending=False)\n",
    "         .head(self.K)\n",
    "         .index\n",
    "         .to_list()\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "        return np.array([feat in self._selected_index_list_ for feat in range(self.n_features_in_)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "grid = \\\n",
    "(GridSearchCV(make_pipeline(SelectKTop(), RandomForestClassifier(random_state=42)),\n",
    "              param_grid={'selectktop__K': np.arange(1,N_FEATURES+1)},\n",
    "              cv= RepeatedStratifiedKFold(n_splits=3, n_repeats=5),\n",
    "              scoring='roc_auc',\n",
    "#               random_state=42, <- como fazer isso no gridsearch?\n",
    "#               refit=False\n",
    "             )\n",
    " .fit(X, y)\n",
    ")\n",
    "\n",
    "df_cv = \\\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    " [['param_selectktop__K', 'mean_test_score', 'std_test_score']]\n",
    ")\n",
    "\n",
    "cv_best = \\\n",
    "(df_cv.\n",
    " sort_values(by='mean_test_score', ascending=False)\n",
    " .reset_index(drop=True)\n",
    " .loc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(df_cv.param_selectktop__K, df_cv.mean_test_score, 1.96*df_cv.std_test_score)\n",
    "plt.scatter(cv_best.param_selectktop__K, cv_best.mean_test_score, s=100)\n",
    "plt.ylim(0.75, 1)\n",
    "plt.xlabel('Número de features selecionadas')\n",
    "plt.xticks(df_cv.param_selectktop__K.astype(int))\n",
    "plt.ylabel('Performance (ROCAUC)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_1', 'column_2', 'column_3', 'column_4', 'column_6',\n",
       "       'column_10', 'column_14'], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose k?\n",
    "\n",
    "It looks quite ad-hoc at the last example.\n",
    "\n",
    "Idea: Create a noise variable that we know is not usefull. We can look at the columns that were better than it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_X = (X.assign(noise_column = np.random.RandomState(42).normal(size=X.shape[0])))\n",
    "noised_X[noised_X.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noised_rfc = RandomForestClassifier(random_state=42).fit(noised_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(list(zip(noised_X.columns, noised_rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .query(f\"feature_importance > {noised_rfc.feature_importances_[-1]}\")\n",
    " .feature_name\n",
    " .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "\n",
    "Different random states or distribution of the random variable we are inputing can make we select different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised2_X = (X.assign(noise_column = np.random.RandomState(42).exponential(size=X.shape[0])))\n",
    "noised2_rfc = RandomForestClassifier(random_state=0).fit(noised2_X, y)\n",
    "\n",
    "(pd.DataFrame(list(zip(noised2_X.columns, noised2_rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .query(f\"feature_importance > {noised_rfc.feature_importances_[-1]}\")\n",
    " .feature_name\n",
    " .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a bad feature can appear and other times, good features can be unlucky and appear bellow the noised one, by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já existem textos que explicam de forma bem didática como o Boruta é calculado [cite]().... Acredito que a construção que fizemos anteriormente complementa as ideias dele.\n",
    "\n",
    "Vamos apenas passar pelas ideiais principais de forma resumida...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta main ideas\n",
    "\n",
    "- Boruta tries to solve this inconsistency repeating the process many times.\n",
    "\n",
    "- At each time, we write down if the feature was better than an noised one or not (in the sense of having better feature importance than it).\n",
    "\n",
    "- For each feature, we then apply an statiscal test to test the hypothesis: *\"does this feature has 50% chance of beeing better than a noised feature?\"*.\n",
    "\n",
    "- The result of this test gives us 3 regions: the ones that we are certain to be better than randomness, the ones that we are certain that are just bad features and the ones we are not confident enough to but in the other classes.\n",
    "\n",
    "- PS: to be fair, Boruta creates the features in an different way than we did in this example. Instead of creating then from scratch, using a new random variable, we just shuffle the columns of the original dataframe. In Boruta literature they are called *shadow variables* instead of *noised*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our discussions solidified the ideas needed for you to understand Boruta in the details. You can dive deeper now with this [excellent blog post](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Boruta\n",
    "\n",
    "The [post](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a) gives a pretty way of using the BorutaPy library. Im just adding some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model we want to use as base estimator\n",
    "\n",
    "- Note that we can add hyper-parameters we find relevant, such as `class_weight`.\n",
    "\n",
    "- When using tree ensembles (let's be honest, always), deeper trees will change slightly the feature_importance methods and will just take longer to compute. In practice, setting `max_depth` as an int is a time saver with not very much loss in performance of the selection because we will be able to set number of boruta trails bigger because of it. Default RandomForests are expanded until all leaves are pure or until all leaves contain less than min_samples_split (default is set to 1) samples which is very computational consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_depth=7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Boruta object and fit it\n",
    "\n",
    "- Boruta's `n_estimators` overwrites the estimator's `n_estimators`. By default, it's set to 1000. If 'auto', then it is determined automatically based on the size of the dataset.\n",
    "- `alpha` and `perc` are parameters you may want to tune a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta = BorutaPy(\n",
    "   estimator = forest,\n",
    "   max_iter = 100, # number of trials to perform\n",
    "   random_state = 42\n",
    ")\n",
    "\n",
    "### fit Boruta (it accepts np.array, not pd.DataFrame)\n",
    "boruta.fit(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the selected features and the ones we are not sure we can safely drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_area = X.columns[boruta.support_].to_list()\n",
    "blue_area = X.columns[boruta.support_weak_].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Selector\n",
    "\n",
    "O primeiro algoritmo que iremos discutir é a técnica de seleção sequencial. A ideia aqui é utilizar uma estratégia gulosa para explorar o número de combinações possíveis de variáveis que temos disponível. A príncipio, temos `2**N_FEATURES` formas diferentes de escolher as `N_FEATURES` disponíveis. Construir todos esses `2**N_FEATURES` modelos é impraticavel. No nosso simples exemplo, com apenas 20 features, teríamos `1048576` combinações diferentes. Se cada modelo demorasse 1 segundo para ser treinado e avaliado, esperaríamos 12 dias até termos todos esses resultados. Com 5 features a mais, teríamos que esperar mais de 1 ano.\n",
    "\n",
    "A ideia da busca gulosa é escolher uma feature de cada vez para ser adicionada (ou removida) da lista de features selecionadas e observar a mudança na performance do modelo.\n",
    "\n",
    "Por exemplo, na direção \"para frente\", podemos comparar o modelo sem nenhuma variável (por exemplo um modelo que retorna alguma estatística dos `y_train`, como a moda, no caso de claissificação, ou uma média/mediana no caso de uma regressão) como todos os `N_FEATURES` modelos possíveis com 1 variável. A variável do modelo que tiver a melhor performance entre os `N_FEATURES` modelos de 1 variável é escolhida para ser a primeira variável selecionada. Para escolher a possível segunda variável, fazemos a mesma coisa comparando \n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs = SequentialFeatureSelector(estimator, direction = \"forward\", n_features_to_select=3)\n",
    "# other optionfor direction: \"backward\"\n",
    "sfs.fit(X, y)\n",
    "feature_mask = sfs.get_support()\n",
    "X_selected_features = sfs.transform(X)\n",
    "```\n",
    "\n",
    "This Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion (one feature at a time).\n",
    "\n",
    "Cons: Even with the greedy approach, it's expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Selector\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThresholdn\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(X)\n",
    "X_selected_features = selector.transform(X)\n",
    "```\n",
    "\n",
    "Feature selector that removes all low-variance features.\n",
    "\n",
    "Cons: which threshold to pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Best Selector\n",
    "\n",
    "SelectKBest is probably the most common technique. We simply select features according to the k highest scores (some measure of feature importances).\n",
    "\n",
    "For instance, you can take the most \"correlated\" features to the target:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "X_selected_features = SelectKBest(mutual_info_regression, k=20).fit_transform(X, y)\n",
    "\n",
    "```\n",
    "\n",
    "In practice, we normaly use it with some model measure of feature importances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
