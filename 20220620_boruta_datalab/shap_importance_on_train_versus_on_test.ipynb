{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11850f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlo\\anaconda3\\envs\\blog_datalab_boruta2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e1d1f",
   "metadata": {},
   "source": [
    "# SHAP importance can be different if we calculate depending if we calculate it with train data or not (even if the data comes from the same distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec13ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 20\n",
    "\n",
    "X, y = \\\n",
    "make_classification(n_samples=1000,\n",
    "                    n_features=N_FEATURES,\n",
    "                    n_informative=2,\n",
    "                    n_redundant=2,\n",
    "                    n_classes=2,\n",
    "                    flip_y=0.1,\n",
    "                    shuffle=False,\n",
    "                    random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f'column_{i+1}' for i in range(N_FEATURES)])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9084af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_df_imp_shap(X, explainer):\n",
    "    shap_vals = explainer.shap_values(X)\n",
    "\n",
    "    df_imp_shap = \\\n",
    "    (pd.DataFrame(list(zip(X.columns, np.abs(shap_vals[0]).mean(axis=0))),\n",
    "                  columns=['feature_name', 'shap_importance'])\n",
    "     .sort_values(by='shap_importance', ascending=False)\n",
    "     .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return df_imp_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07b9701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>shap_importance_train</th>\n",
       "      <th>shap_importance_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.208237</td>\n",
       "      <td>0.208660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.097596</td>\n",
       "      <td>0.096554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>0.034153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>column_1</td>\n",
       "      <td>0.033912</td>\n",
       "      <td>0.035134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>column_6</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.009675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>column_10</td>\n",
       "      <td>0.011362</td>\n",
       "      <td>0.009502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>column_9</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.010362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>column_8</td>\n",
       "      <td>0.010399</td>\n",
       "      <td>0.008021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>column_11</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.010040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>column_12</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.008552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>column_7</td>\n",
       "      <td>0.009029</td>\n",
       "      <td>0.008756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>0.006472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>column_19</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>0.006539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>column_17</td>\n",
       "      <td>0.006839</td>\n",
       "      <td>0.005579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.005389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>column_14</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.005396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>column_15</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.005471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_13</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.005047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_5</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.004409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_18</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>0.004288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  shap_importance_train  shap_importance_test\n",
       "0      column_2               0.208237              0.208660\n",
       "1      column_3               0.097596              0.096554\n",
       "2      column_4               0.038523              0.034153\n",
       "3      column_1               0.033912              0.035134\n",
       "4      column_6               0.012097              0.009675\n",
       "5     column_10               0.011362              0.009502\n",
       "6      column_9               0.010714              0.010362\n",
       "7      column_8               0.010399              0.008021\n",
       "8     column_11               0.010168              0.010040\n",
       "9     column_12               0.009237              0.008552\n",
       "10     column_7               0.009029              0.008756\n",
       "11    column_16               0.007649              0.006472\n",
       "12    column_19               0.007613              0.006539\n",
       "13    column_17               0.006839              0.005579\n",
       "14    column_20               0.006551              0.005389\n",
       "15    column_14               0.006463              0.005396\n",
       "16    column_15               0.006150              0.005471\n",
       "17    column_13               0.006068              0.005047\n",
       "18     column_5               0.005326              0.004409\n",
       "19    column_18               0.005159              0.004288"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(rfc)\n",
    "\n",
    "df_imp_shap_train = return_df_imp_shap(X_train, explainer)\n",
    "df_imp_shap_test = return_df_imp_shap(X_test, explainer)\n",
    "\n",
    "df_imp_shap_train_test = \\\n",
    "df_imp_shap_train.merge(df_imp_shap_test, \n",
    "                        on='feature_name',\n",
    "                        suffixes = ('_train', '_test')\n",
    "                       )\n",
    "\n",
    "df_imp_shap_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76862766",
   "metadata": {},
   "source": [
    "# It's close, but different enough to lead to different sorting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68101ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['column_18', 'column_5', 'column_13', 'column_15', 'column_14',\n",
       "        'column_20', 'column_17', 'column_19', 'column_16', 'column_7',\n",
       "        'column_12', 'column_11', 'column_8', 'column_9', 'column_10',\n",
       "        'column_6', 'column_1', 'column_4', 'column_3', 'column_2'],\n",
       "       dtype='<U9'),\n",
       " array(['column_18', 'column_5', 'column_13', 'column_20', 'column_14',\n",
       "        'column_15', 'column_17', 'column_16', 'column_19', 'column_8',\n",
       "        'column_12', 'column_7', 'column_10', 'column_6', 'column_11',\n",
       "        'column_9', 'column_4', 'column_1', 'column_3', 'column_2'],\n",
       "       dtype='<U9'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(df_imp_shap_train\n",
    "  .sort_values(by='shap_importance')\n",
    "  .feature_name\n",
    "  .to_list())\n",
    ",\n",
    " np.array(df_imp_shap_test\n",
    "  .sort_values(by='shap_importance')\n",
    "  .feature_name\n",
    "  .to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbe519",
   "metadata": {},
   "source": [
    "# Reserving a slice of your dataset to calculate SHAP importance\n",
    "\n",
    "For instance, we could calculate the SHAP importance using the test data. This [`XSHAPImportanceRandomForestClassifier`](?) classifier I implemented holds the dataset `X_shap` when we instantiate the class and is used when we ask for it's `feature_importances_` as you can see from the [source code here](?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a5dd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.035134  , 0.20866036, 0.09655425, 0.03415335, 0.00440922,\n",
       "       0.00967484, 0.00875638, 0.00802079, 0.0103625 , 0.00950239,\n",
       "       0.01004044, 0.00855213, 0.00504655, 0.0053958 , 0.00547095,\n",
       "       0.00647194, 0.00557922, 0.00428836, 0.00653862, 0.00538897])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shap_feature_importances_ import XSHAPImportanceRandomForestClassifier\n",
    "\n",
    "rfc_shap = XSHAPImportanceRandomForestClassifier(random_state=42, X_shap=X_test).fit(X_train, y_train)\n",
    "rfc_shap.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog_datalab_boruta2",
   "language": "python",
   "name": "blog_datalab_boruta2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
