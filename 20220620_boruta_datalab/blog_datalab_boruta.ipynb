{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST COMPLEMENTAR PARA O MEU BLOG: ESTRATÉGIAS DE FEATURE SELECTION DO KSLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlo\\anaconda3\\envs\\blog_datalab_boruta2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boruta Feature Selection\n",
    "\n",
    "Ter menos atributos nos retorna um modelo menos complexo. Para ilustrar essa afirmação, temos que a [dimensão-VC](https://youtu.be/Dc0sr0kdBVI) (medida de complexidade de uma família de hipóteses) de um perceptron (classificador linear) é $d+1$ em que $d$ é a número de variáveis. Na prática, isso significa, menos chance de overfitting e, consequentemente, nos dá modelos mais estáveis e com melhor performance fora do laboratório. Por isso, na minha opinião, a seleção de variáveis é uma das mais poderosas ferramentas data-centric.\n",
    "\n",
    "Entretanto, esse assunto não é visto com o cuidado devido na maioria dos cursos de Aprendizado de Máquina. São apresentados poucos métodos e de maneira superficial. Focando ainda em técnicas que não são escaláveis com o aumento de variáveis e por isso são impraticáveis (como as estratégias gulosas de [`sklearn.feature_selection.SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)).\n",
    "\n",
    "No DataLab, seleção de variáveis se torna extremamente relevante pela natureza dos problemas que trabalhamos. Na grande maioria dos casos temos algumas milhares de variáveis do Bureau da Serasa e não é fácil identificar de antemão quais serão as que nos darão mais ganhos. É necessário aplicar técnicas que são robustas à grandeza de variáveis que temos ao mesmo tempo que garantimos uma seleção que faça sentido.\n",
    "\n",
    "Neste post iremos motivar a construção do Boruta, uma das técnicas mais utilizadas pelos cientistas do DataLab na seleção de features, com algumas dicas de uso prático. Ilustraremos ainda o uso da função [`boruta.BorutaPy`](https://github.com/scikit-learn-contrib/boruta_py), do ambiente [scikit-learn-contrib](https://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md) (ou seja, compátivel com qualquer biblioteca que siga o [padrão de código do scikit-learn](https://scikit-learn.org/stable/developers/develop.html).\n",
    "\n",
    "___\n",
    "\n",
    "Para ilustrar o problema de seleção de features, utilizaremos o [`sklearn.datasets.make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) para criar um problema genérico de classificação em que podemos definir, como um parâmetro da função, o número de variáveis úteis para o problema de previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>...</th>\n",
       "      <th>column_17</th>\n",
       "      <th>column_18</th>\n",
       "      <th>column_19</th>\n",
       "      <th>column_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.050478</td>\n",
       "      <td>-1.323568</td>\n",
       "      <td>0.912474</td>\n",
       "      <td>1.009796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800511</td>\n",
       "      <td>1.238946</td>\n",
       "      <td>0.209659</td>\n",
       "      <td>-0.491636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.580834</td>\n",
       "      <td>-2.747104</td>\n",
       "      <td>1.777419</td>\n",
       "      <td>1.850430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524088</td>\n",
       "      <td>0.152355</td>\n",
       "      <td>-0.822420</td>\n",
       "      <td>1.121031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.885704</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>0.631813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262561</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>-0.137372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.525438</td>\n",
       "      <td>-2.967793</td>\n",
       "      <td>1.884777</td>\n",
       "      <td>1.924410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617652</td>\n",
       "      <td>-0.316073</td>\n",
       "      <td>0.615771</td>\n",
       "      <td>1.203884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.076826</td>\n",
       "      <td>-1.014619</td>\n",
       "      <td>0.752233</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326745</td>\n",
       "      <td>0.300474</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>-1.138833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   column_1  column_2  column_3  column_4  ...  column_17  column_18  \\\n",
       "0 -1.050478 -1.323568  0.912474  1.009796  ...   1.800511   1.238946   \n",
       "1 -1.580834 -2.747104  1.777419  1.850430  ...  -0.524088   0.152355   \n",
       "2 -0.885704 -0.614600  0.501004  0.631813  ...   0.262561   0.193590   \n",
       "3 -1.525438 -2.967793  1.884777  1.924410  ...  -0.617652  -0.316073   \n",
       "4 -1.076826 -1.014619  0.752233  0.885267  ...   0.326745   0.300474   \n",
       "\n",
       "   column_19  column_20  \n",
       "0   0.209659  -0.491636  \n",
       "1  -0.822420   1.121031  \n",
       "2   0.850898  -0.137372  \n",
       "3   0.615771   1.203884  \n",
       "4   0.622207  -1.138833  \n",
       "\n",
       "[5 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "N_FEATURES = 20\n",
    "\n",
    "X, y = \\\n",
    "make_classification(n_samples=1000,\n",
    "                    n_features=N_FEATURES,\n",
    "                    n_informative=2,\n",
    "                    n_redundant=2,\n",
    "                    n_classes=2,\n",
    "                    flip_y=0.1,\n",
    "                    shuffle=False,\n",
    "                    random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f'column_{i+1}' for i in range(N_FEATURES)])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train...\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos escolhendo 2 features informativas e 2 features redundantes, temos que as 4 features mais importantes são as colunas: column_1, column_2, column_3 e column_4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção do Boruta\n",
    "\n",
    "## Medindo a importância de uma variável\n",
    "\n",
    "Uma das técnicas mais comuns para selecionar as variáveis é aproveitar-se de modelos que de alguma forma selecionam as variáveis no processo de treinamento. Árvores e, consequentemente, cômites de árvores são talvez o melhor exemplo disso: pela estratégia gulosa de fazer sempre a melhor quebra possível (de acordo com algum critério de melhor, usualmente relacionada a pureza das folhas criadas), estamos sempre escolhendo a melhor quebra da melhor variável para ser feita naquela etapa. Variáveis pouco discriminativas são escolhidas muito menos que as variáveis que de fato ajudam a fazer a previsão.\n",
    "\n",
    "Esse processo naturalmente deriva medidas de importâncias para as variáveis como: o número de vezes que ela é utilizada (esse é o modo default do atributo `feature_importance_` dos ensembles do LGBM, como o [`lightgbm.LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html)) ou uma ponderação do ganho de critério no processo de escolha das quebras das features (essa é a forma default dos ensembles de árvores do sklearn, como o [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), o [`sklearn.ensemble.ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html), e o [`sklearn.ensemble.HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) e também vira o atributo do LGBM quando setamos o `importance_type='gain'`).\n",
    "\n",
    "Com essa medida natural de importância, é razoável ordenar nossas variáveis da mais importante para a menos importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.278748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.201150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.092612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.018641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_18</td>\n",
       "      <td>0.017565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.016912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  feature_importance\n",
       "0      column_2            0.278748\n",
       "1      column_3            0.201150\n",
       "2      column_4            0.092612\n",
       "..          ...                 ...\n",
       "17    column_16            0.018641\n",
       "18    column_18            0.017565\n",
       "19    column_20            0.016912\n",
       "\n",
       "[20 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42).fit(X, y)\n",
    "\n",
    "df_imp = \\\n",
    "(pd.DataFrame(list(zip(X.columns, rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$\\oint$ Existem algumas outras formas de metrificar a importância de uma variável, como por exemplo utilizando suas contribuições de [valores SHAP](https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30). Como o [`shap.Explainer(model).shap_values(X)`](https://github.com/slundberg/shap) nos retorna uma medida de quanto aquela variável agregou na previsão, pegar a sua média entre todos os exemplos nos dá uma forma de ver o quão útil ela foi para discriminar os exemplos como um todo. Para os valores não se cancelarem (imagine uma variável que para determinados valores joga a previsão para cima e em outros valores joga a previsão para baixo), tomamos o módulo antes de fazer a média. Repare que a ordem das importâncias dada pelo SHAP pode ser diferente da ordem de importâncias dada pelo atributo de `feature_importance_` usual do estimador, como é o caso do nosso exemplo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>shap_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.197645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.107211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.043797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_5</td>\n",
       "      <td>0.005099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  shap_importance\n",
       "0      column_2         0.197645\n",
       "1      column_3         0.107211\n",
       "2      column_4         0.043797\n",
       "..          ...              ...\n",
       "17    column_16         0.005268\n",
       "18     column_5         0.005099\n",
       "19    column_20         0.005019\n",
       "\n",
       "[20 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(rfc)\n",
    "shap_vals = explainer.shap_values(X)\n",
    "\n",
    "df_imp_shap = \\\n",
    "(pd.DataFrame(list(zip(X.columns, np.abs(shap_vals[0]).mean(axis=0))),\n",
    "              columns=['feature_name', 'shap_importance'])\n",
    " .sort_values(by='shap_importance', ascending=False)\n",
    " .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_imp_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não falamos do Boruta, mas ele se utiliza dessa ordenação para fazer suas análises e é implementado, usualmente utilizando medida de importância do estimador. Essa diferença motivou alguns contribuidores a implementar o Boruta-Shap. Infelizmente a biblioteca não é tão bem estruturada quanto a do Boruta, mas pode ser uma opção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionando as `K` melhores\n",
    "\n",
    "Se queremos que nosso modelo tenha apenas as `K` features mais úteis, a maneira natural de escolher elas seria pegar as `K` variáveis com maiores valores de importância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column_2', 'column_3', 'column_4', 'column_1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 4\n",
    "\n",
    "(df_imp\n",
    " .head(K)\n",
    " .feature_name\n",
    " .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma das estratégias mais comuns de se fazer seleção de features no mercado, mas levanta algumas questões. A primeira e mais imediata é: como escolher o número de variáveis `K` ideal. Nesse caso ilustrativo, sabemos que 4 variáveis é o número correto, mas na maioria dos casos de aplicação real é irrealista ter esse número de antemão.\n",
    "\n",
    "*$\\oint$ Uma estratégia muito utilizada, mas que não vamos focar muito, é aumentar a lista de features do modelo seguindo a ordenação dada pelo modelo treinado em todas as features. Encarando esse valor `K` como um hiper-parâmetro que estamos otimizando. No exemplo abaixo, fazemos isso de utilizando o [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ao construir uma classe `SelectKTop` utilizando o padrão necessário para os selecionadores de variáveis do scikit-learn, isto é, seguindo a forma que o [`sklearn.feature_selection.SelectorMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectorMixin.html) exige.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deixar essa célular inteira num .py separado do post (com link para o interessado clicar)?\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class SelectKTop(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Controls 3 sources of randomness:\n",
    "        - the bootstrapping of the samples used when building trees\n",
    "          (if ``bootstrap=True``)\n",
    "        - the sampling of the features to consider when looking for the best\n",
    "          split at each node (if ``max_features < n_features``)\n",
    "        - the draw of the splits for each of the `max_features`\n",
    "        See :term:`Glossary <random_state>` for details.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, K=5, base_estimator=None, random_state=None):\n",
    "        self.K = K\n",
    "        self.base_estimator = base_estimator\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _validate_estimator(self, default):\n",
    "        if self.base_estimator is not None:\n",
    "            self.base_estimator_ = self.base_estimator\n",
    "        else:\n",
    "            self.base_estimator_ = default\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = self._validate_data(\n",
    "            X,\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "        \n",
    "        self._validate_estimator(RandomForestClassifier(random_state=self.random_state))\n",
    "        \n",
    "        self.base_estimator_.fit(X, y)\n",
    "        \n",
    "        self._selected_index_list_ = \\\n",
    "        (pd.DataFrame(list(zip(range(self.n_features_in_), self.base_estimator_.feature_importances_)),\n",
    "                      columns=['feature_name', 'feature_importance'])\n",
    "         .sort_values(by='feature_importance', ascending=False)\n",
    "         .head(self.K)\n",
    "         .index\n",
    "         .to_list()\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "        return np.array([feat in self._selected_index_list_ for feat in range(self.n_features_in_)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "grid = \\\n",
    "(GridSearchCV(make_pipeline(SelectKTop(random_state=42), \n",
    "                            RandomForestClassifier(random_state=42)),\n",
    "              param_grid={'selectktop__K': np.arange(1,N_FEATURES+1)},\n",
    "              cv= RepeatedStratifiedKFold(n_splits=3, \n",
    "                                          n_repeats=1, \n",
    "                                          random_state=42),\n",
    "              scoring='roc_auc',\n",
    "#               verbose=3\n",
    "             )\n",
    " .fit(X, y)\n",
    ")\n",
    "\n",
    "df_cv = \\\n",
    "(pd.DataFrame(grid.cv_results_)[['param_selectktop__K', \n",
    "                                 'mean_test_score', \n",
    "                                 'std_test_score']])\n",
    "\n",
    "cv_best = \\\n",
    "(df_cv.\n",
    " sort_values(by='mean_test_score', ascending=False)\n",
    " .reset_index(drop=True)\n",
    " .loc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(df_cv.param_selectktop__K, df_cv.mean_test_score, 1.96*df_cv.std_test_score)\n",
    "plt.scatter(cv_best.param_selectktop__K, cv_best.mean_test_score, s=100)\n",
    "plt.ylim(0.75, 1)\n",
    "plt.xlabel('Número de features selecionadas')\n",
    "plt.xticks(df_cv.param_selectktop__K.astype(int))\n",
    "plt.ylabel('Performance (ROCAUC)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No nosso experimento controlado, encontramos algumas poucas variáveis a mais do que o correto.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_1', 'column_2', 'column_3', 'column_4', 'column_6',\n",
       "       'column_10'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vale citar que esse método pode ser deixado mais robusto variando o `random_seed` do `base_estimator` e tendo uma distribuição de importâncias para cada variável ao invés de apenas um valor único (que é mais ruídoso). Além disso, recomendo utilizar SHAP para medir a importância nesse caso.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionando as `K` melhores incluindo uma variável aleatória\n",
    "\n",
    "Ao criar uma variável de ruído, ou seja, que sabidamente não é útil para a previsão, teremos um ponto de corte para filtro das variáveis que demonstram ajudar na previsão. A ideia dessa abordagem é medir a importância da variável aleatória e ficar apenas com variáveis que se demonstraram mais importantes.\n",
    "\n",
    "Adicionando a nova coluna, por exemplo, amostrada de uma variável aleatória $\\mathcal{N}(0,1)$ de forma independente, temos uma nova lista de importância das variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_column</th>\n",
       "      <th>column_20</th>\n",
       "      <th>column_19</th>\n",
       "      <th>column_18</th>\n",
       "      <th>...</th>\n",
       "      <th>column_4</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.491636</td>\n",
       "      <td>0.209659</td>\n",
       "      <td>1.238946</td>\n",
       "      <td>...</td>\n",
       "      <td>1.009796</td>\n",
       "      <td>0.912474</td>\n",
       "      <td>-1.323568</td>\n",
       "      <td>-1.050478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.138264</td>\n",
       "      <td>1.121031</td>\n",
       "      <td>-0.822420</td>\n",
       "      <td>0.152355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.850430</td>\n",
       "      <td>1.777419</td>\n",
       "      <td>-2.747104</td>\n",
       "      <td>-1.580834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>-0.137372</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631813</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>-0.885704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.523030</td>\n",
       "      <td>1.203884</td>\n",
       "      <td>0.615771</td>\n",
       "      <td>-0.316073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.924410</td>\n",
       "      <td>1.884777</td>\n",
       "      <td>-2.967793</td>\n",
       "      <td>-1.525438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-1.138833</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.300474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>0.752233</td>\n",
       "      <td>-1.014619</td>\n",
       "      <td>-1.076826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   noise_column  column_20  column_19  column_18  ...  column_4  column_3  \\\n",
       "0      0.496714  -0.491636   0.209659   1.238946  ...  1.009796  0.912474   \n",
       "1     -0.138264   1.121031  -0.822420   0.152355  ...  1.850430  1.777419   \n",
       "2      0.647689  -0.137372   0.850898   0.193590  ...  0.631813  0.501004   \n",
       "3      1.523030   1.203884   0.615771  -0.316073  ...  1.924410  1.884777   \n",
       "4     -0.234153  -1.138833   0.622207   0.300474  ...  0.885267  0.752233   \n",
       "\n",
       "   column_2  column_1  \n",
       "0 -1.323568 -1.050478  \n",
       "1 -2.747104 -1.580834  \n",
       "2 -0.614600 -0.885704  \n",
       "3 -2.967793 -1.525438  \n",
       "4 -1.014619 -1.076826  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_X = (X.assign(noise_column = np.random.RandomState(42).normal(size=X.shape[0])))\n",
    "noised_X[noised_X.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.266446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.205667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.087548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>column_5</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_19</td>\n",
       "      <td>0.018264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.017692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  feature_importance\n",
       "1      column_2            0.266446\n",
       "2      column_3            0.205667\n",
       "3      column_4            0.087548\n",
       "..          ...                 ...\n",
       "4      column_5            0.018706\n",
       "18    column_19            0.018264\n",
       "19    column_20            0.017692\n",
       "\n",
       "[21 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_rfc = RandomForestClassifier(random_state=42).fit(noised_X, y)\n",
    "\n",
    "df_imp_normal_noise = \\\n",
    "(pd.DataFrame(list(zip(noised_X.columns, noised_rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    ")\n",
    "\n",
    "df_imp_normal_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a última variável é a nossa coluna sabidamente ruídosa, a ideia dessa técnica é selecionar apenas as variáveis que tem importância maior do que o limiar settado pela importância da variável não relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_2', 'column_3', 'column_4', 'column_1', 'column_6',\n",
       "       'column_10', 'column_14'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(\n",
    " df_imp_normal_noise\n",
    " .query(f\"feature_importance > {noised_rfc.feature_importances_[-1]}\")\n",
    " .feature_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale observar que a escolha da variável ruídosa como $\\mathcal{N}(0,1)$ foi totalmente arbitrária. Entretanto isso faz diferença e pode fazer a seleção de variáveis ser diferente. No nosso exemplo controlado, mudar o ruído para $\\textrm{Exp}(1)$ nos faria selecionar variáveis finais diferentes, totalmente por sorte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_2', 'column_3', 'column_4', 'column_1', 'column_14',\n",
       "       'column_6', 'column_10', 'column_9'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised2_X = (X.assign(noise_column = np.random.RandomState(42).exponential(size=X.shape[0])))\n",
    "noised2_rfc = RandomForestClassifier(random_state=0).fit(noised2_X, y)\n",
    "\n",
    "np.array(\n",
    " pd.DataFrame(list(zip(noised2_X.columns, noised2_rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .query(f\"feature_importance > {noised_rfc.feature_importances_[-1]}\")\n",
    " .feature_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso nos demonstra um problema desse método. Apesar de poderoso, por nos dar um jeito interessante de selecionar as variáveis sem escolher `K` de forma arbitrária, a escolha da distribuição da variável ruídosa é uma fonte de variação relevante.\n",
    "\n",
    "Em muitos casos, ter variáveis discretas *versus* contínuas pode influenciar na medida de importância (como é o caso de árvores que, por ter mais quebras disponíveis, vai ter mais chance de escolher uma variável ruídosa) ou a própria escala da feature adicionada pode atrapalhar nessa mensuração (por exemplo, por estarmos usando os coefiencientes angulares de um [`sklearn.linear_model.Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)).\n",
    "\n",
    "Toda essa variabilidade pode fazer uma *feature* ruim ser selecionada as vezes e em outras vezes uma variável boa ser discartada simplemente por azar.\n",
    "\n",
    "O Boruta vem para tentar lidar com essas duas questões ao mesmo tempo: tentar manter a distribuição marginal das features ruídosas iguais às distribuições marginais das features originais, enquanto tenta ser robusto à variabilidade, repetindo o experimento algumas vezes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta\n",
    "\n",
    "Já existem muitos textos úteis que explicam o Boruta de forma didática e com exemplos. Como a ideia desse post não é ser redundante com a literatura e sim compilar ideias centrais de uso prático, vamos apenas citar os principais aspectos e deixar o convite para uma leitura detalhada de outras referências do tema como o post [Boruta Explained Exactly How You Wished Someone Explained to You](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a). A construção que fizemos anteriormente vão deixar as ideias do Boruta ainda mais claras, justificando o modo de serem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta main ideas\n",
    "\n",
    "- Boruta tries to solve this inconsistency repeating the process many times.\n",
    "\n",
    "- At each time, we write down if the feature was better than an noised one or not (in the sense of having better feature importance than it).\n",
    "\n",
    "- For each feature, we then apply an statiscal test to test the hypothesis: *\"does this feature has 50% chance of beeing better than a noised feature?\"*.\n",
    "\n",
    "- The result of this test gives us 3 regions: the ones that we are certain to be better than randomness, the ones that we are certain that are just bad features and the ones we are not confident enough to but in the other classes.\n",
    "\n",
    "- PS: to be fair, Boruta creates the features in an different way than we did in this example. Instead of creating then from scratch, using a new random variable, we just shuffle the columns of the original dataframe. In Boruta literature they are called *shadow variables* instead of *noised*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our discussions solidified the ideas needed for you to understand Boruta in the details. You can dive deeper now with this [excellent blog post](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Boruta\n",
    "\n",
    "The [post](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a) gives a pretty way of using the BorutaPy library. Im just adding some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model we want to use as base estimator\n",
    "\n",
    "- Note that we can add hyper-parameters we find relevant, such as `class_weight`.\n",
    "\n",
    "- When using tree ensembles (let's be honest, always), deeper trees will change slightly the feature_importance methods and will just take longer to compute. In practice, setting `max_depth` as an int is a time saver with not very much loss in performance of the selection because we will be able to set number of boruta trails bigger because of it. Default RandomForests are expanded until all leaves are pure or until all leaves contain less than min_samples_split (default is set to 1) samples which is very computational consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_depth=7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Boruta object and fit it\n",
    "\n",
    "- Boruta's `n_estimators` overwrites the estimator's `n_estimators`. By default, it's set to 1000. If 'auto', then it is determined automatically based on the size of the dataset.\n",
    "- `alpha` and `perc` are parameters you may want to tune a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta = BorutaPy(\n",
    "   estimator = forest,\n",
    "   max_iter = 100, # number of trials to perform\n",
    "   random_state = 42\n",
    ")\n",
    "\n",
    "### fit Boruta (it accepts np.array, not pd.DataFrame)\n",
    "boruta.fit(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the selected features and the ones we are not sure we can safely drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_area = X.columns[boruta.support_].to_list()\n",
    "blue_area = X.columns[boruta.support_weak_].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Selector\n",
    "\n",
    "O primeiro algoritmo que iremos discutir é a técnica de seleção sequencial. A ideia aqui é utilizar uma estratégia gulosa para explorar o número de combinações possíveis de variáveis que temos disponível. A príncipio, temos `2**N_FEATURES` formas diferentes de escolher as `N_FEATURES` disponíveis. Construir todos esses `2**N_FEATURES` modelos é impraticavel. No nosso simples exemplo, com apenas 20 features, teríamos `1048576` combinações diferentes. Se cada modelo demorasse 1 segundo para ser treinado e avaliado, esperaríamos 12 dias até termos todos esses resultados. Com 5 features a mais, teríamos que esperar mais de 1 ano.\n",
    "\n",
    "A ideia da busca gulosa é escolher uma feature de cada vez para ser adicionada (ou removida) da lista de features selecionadas e observar a mudança na performance do modelo.\n",
    "\n",
    "Por exemplo, na direção \"para frente\", podemos comparar o modelo sem nenhuma variável (por exemplo um modelo que retorna alguma estatística dos `y_train`, como a moda, no caso de claissificação, ou uma média/mediana no caso de uma regressão) como todos os `N_FEATURES` modelos possíveis com 1 variável. A variável do modelo que tiver a melhor performance entre os `N_FEATURES` modelos de 1 variável é escolhida para ser a primeira variável selecionada. Para escolher a possível segunda variável, fazemos a mesma coisa comparando \n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs = SequentialFeatureSelector(estimator, direction = \"forward\", n_features_to_select=3)\n",
    "# other optionfor direction: \"backward\"\n",
    "sfs.fit(X, y)\n",
    "feature_mask = sfs.get_support()\n",
    "X_selected_features = sfs.transform(X)\n",
    "```\n",
    "\n",
    "This Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion (one feature at a time).\n",
    "\n",
    "Cons: Even with the greedy approach, it's expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Selector\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThresholdn\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(X)\n",
    "X_selected_features = selector.transform(X)\n",
    "```\n",
    "\n",
    "Feature selector that removes all low-variance features.\n",
    "\n",
    "Cons: which threshold to pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Best Selector\n",
    "\n",
    "SelectKBest is probably the most common technique. We simply select features according to the k highest scores (some measure of feature importances).\n",
    "\n",
    "For instance, you can take the most \"correlated\" features to the target:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "X_selected_features = SelectKBest(mutual_info_regression, k=20).fit_transform(X, y)\n",
    "\n",
    "```\n",
    "\n",
    "In practice, we normaly use it with some model measure of feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectorMixin pro variável ruído:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.49014246,  9.5852071 , 11.94306561, 14.56908957,  9.29753988,\n",
       "        9.29758913, 14.73763845, 12.30230419,  8.59157684, 11.62768013,\n",
       "        8.60974692,  8.60281074, 10.72588681,  4.26015927,  4.8252465 ,\n",
       "        8.31313741,  6.96150664, 10.942742  ,  7.27592777,  5.7630889 ,\n",
       "       14.39694631,  9.3226711 , 10.20258461,  5.72575544,  8.36685183,\n",
       "       10.33276777,  6.54701927, 11.12709406,  8.19808393,  9.12491875,\n",
       "        8.19488016, 15.55683455,  9.95950833,  6.82686721, 12.46763474,\n",
       "        6.33746905, 10.62659079,  4.12098963,  6.01544185, 10.59058371,\n",
       "       12.21539974, 10.51410484,  9.65305515,  9.09668891,  5.56443403,\n",
       "        7.84046737,  8.61808369, 13.17136668, 11.03085487,  4.71087953,\n",
       "       10.97225191,  8.84475316,  7.969234  , 11.83502887, 13.09299857,\n",
       "       12.79384036,  7.48234743,  9.07236287, 10.99379029, 12.92663538,\n",
       "        8.56247729,  9.44302307,  6.68099508,  6.41138013, 12.43757747,\n",
       "       14.06872009,  9.78396964, 13.01059869, 11.08490808,  8.06464074,\n",
       "       11.08418682, 14.6141097 ,  9.89252188, 14.69393097,  2.14076469,\n",
       "       12.46570751, 10.2611412 ,  9.10297795, 10.27528233,  4.03729326,\n",
       "        9.34098434, 11.07133771, 14.43368213,  8.44518935,  7.57451919,\n",
       "        8.49472887, 12.74620635, 10.98625333,  8.41071939, 11.5398023 ,\n",
       "       10.29123265, 12.90593497,  7.89384072,  9.01701356,  8.82367554,\n",
       "        5.60945516, 10.88836083, 10.78316582, 10.01534037,  9.2962386 ,\n",
       "        5.75388777,  8.73806403,  8.97185645,  7.59316819,  9.51614287,\n",
       "       11.21215257, 15.6585577 , 10.52373344, 10.77265117,  9.77666225,\n",
       "        4.24368635,  9.92045837, 10.18069063, 17.38972634,  9.42291711,\n",
       "       10.90464203,  9.89586469,  6.49396589, 13.42846844, 12.2557991 ,\n",
       "       12.37309584,  7.27183764, 14.20838293,  5.79444681, 11.76057128,\n",
       "       16.57136688,  7.02839102,  8.30110681, 10.2989541 ,  8.48957304,\n",
       "        5.34800971, 10.20568892,  6.81308886, 11.42077729,  7.2417273 ,\n",
       "       14.64980322,  7.65024012,  9.03381545, 12.44055165,  6.30740705,\n",
       "       10.6823798 , 13.92142826,  5.1775503 , 10.55390158, 10.77964838,\n",
       "       12.34546862,  6.28914787,  6.03863016, 11.5658247 , 10.89095402,\n",
       "       10.75147855, 11.03934463,  7.95992584, 10.69676109, 10.87921742,\n",
       "        7.85694575, 15.59732353, 11.42149876,  6.42608951, 11.96966083,\n",
       "        7.07595499, 12.36125381, 13.47578674,  7.53795304, 12.89012839,\n",
       "       11.23834278, 12.46618048, 15.69037895,  9.26383565,  7.73879151,\n",
       "        7.33145671,  7.55256915,  9.76869487, 11.02345592, 10.8300724 ,\n",
       "       12.48154975, 10.03900568, 14.36060223,  9.2060295 , 18.1605075 ,\n",
       "       11.87700204,  7.42852733,  6.78732251, 11.44741725,  9.32961164,\n",
       "       12.14200148, 11.41971287,  9.78151326,  7.45961885,  5.45545833,\n",
       "        8.66045514, 12.56919638, 10.64228123,  6.26278366, 10.51954278,\n",
       "       11.15595214,  7.34842769, 10.46117532, 10.17462616,  6.57108911,\n",
       "       11.07336208, 11.68235358, 13.24915373, 13.16140616,  5.8669919 ,\n",
       "        7.18652488, 11.5451058 , 11.54135785, 11.54514306, 21.55819447,\n",
       "       11.71267153, 13.40669692, 12.86200529, 11.95417375,  9.05419227,\n",
       "       12.27690766,  7.68152436,  9.28954418,  8.54390936, 10.24562242,\n",
       "       16.9439757 ,  4.39820442, 12.05878057,  5.16185239,  8.5842044 ,\n",
       "       13.26685179, 10.19284006,  6.76676567,  7.85408887, 12.03879325,\n",
       "        7.8089001 , 10.64937577, 10.13671552,  8.04519896, 16.43183227,\n",
       "       11.90175707,  3.92457224, 10.55936294,  8.01464061, 12.5573    ,\n",
       "        7.62243778,  9.65579068, 11.51496184, 12.59726558,  6.39911078,\n",
       "        8.99649629,  8.57516407,  8.0400123 , 15.29636272, 11.21494513,\n",
       "        6.21734814, 12.75358584, 16.36646859, 13.09739578,  5.4418901 ,\n",
       "        8.54729778, 13.80073345,  7.8769916 , 11.33145828, 12.32390216,\n",
       "        7.21920859,  9.82142393,  0.27619798,  6.92683708,  9.24229555,\n",
       "        6.25665045, 14.89723391,  5.70957587,  8.67986654, 10.39222173,\n",
       "       14.32381987,  5.69241355, 13.48949126, 10.03069918,  7.05547405,\n",
       "       11.38631042, 10.59717909,  8.19934937, 10.20940625,  8.84405921,\n",
       "       10.34055204, 11.98639202, 14.75805045,  6.2865535 , 16.39910012,\n",
       "        4.1437366 ,  9.54464471, 11.76495162, 10.8429756 ,  8.13190144,\n",
       "        9.37563325,  8.5209972 ,  8.23190573, 12.54880629, 11.07104646,\n",
       "        7.92127121, 12.69879963, 10.92189856, 12.43858636, 11.88888653,\n",
       "        7.51301497,  8.31945688, 12.24188082, 11.8311108 ,  9.93729522,\n",
       "       10.35198215, 13.83299469,  8.22528583, 11.64129214,  9.39342204,\n",
       "        9.34695639, 13.29633056, 12.47624905, 12.44052891, 13.91643642,\n",
       "       10.06301152, 12.04585891,  9.06919973, 10.97249906,  9.60957084,\n",
       "       10.29098789, 11.78547108,  7.54533795, 16.27716183,  6.98194786,\n",
       "        6.35743416, 13.47433262, 12.37498808, 11.87235945, 11.88503653,\n",
       "        9.96325968,  7.30823689, 10.22741367,  7.96851487, 12.9253592 ,\n",
       "        9.55882786,  7.52350841,  9.03584248, 11.23879436,  8.30882634,\n",
       "        7.53333881, 10.73106163, 10.73489971,  8.47917047,  8.58688508,\n",
       "       10.69614981,  5.65574698,  5.77760868,  7.84466734,  9.35965854,\n",
       "       10.9327227 , 14.42606865, 12.57297887,  9.52018441,  9.94295138,\n",
       "        6.99241191,  9.94446059,  9.13402408, 10.96815568,  7.51830717,\n",
       "       11.55803954, 14.59821674,  9.67371955, 11.20513517, 12.07043198,\n",
       "        8.79633858, 10.67227745, 10.0377772 , 10.2930283 ,  7.68097065,\n",
       "       10.07353052, 11.49399487, 14.35343082, 12.87781248, 16.45954737,\n",
       "        7.69795731, 12.61696191, 10.55002602, 16.5694088 ,  7.57510514,\n",
       "        7.48083447,  8.20182206,  3.62831283,  8.42273493,  7.72260202,\n",
       "       10.45118136, 11.02526793, 15.62851252, 12.85127151,  8.26928903,\n",
       "        7.30475599, 11.47575751,  6.03930038, 15.4943763 , 13.53832036,\n",
       "        8.59247304,  4.86059641, 14.06161712,  9.65638046, 13.71344894,\n",
       "        5.21671702,  8.20187493, 10.0157311 , 10.14094178,  8.64980359,\n",
       "       11.8685498 ,  6.79713871,  9.57286154, 10.3608869 , 11.5433165 ,\n",
       "       12.13484463,  6.62607372,  5.39765749, 13.83303047, 10.99694204,\n",
       "        7.75454039, 14.65345593, 10.3470239 , 13.53789155, 10.20255544,\n",
       "       16.18224377, 15.26602253,  9.25310755, 12.91471285, 11.93612785,\n",
       "       14.10589467,  7.10522962, 12.05815438, 13.17527346,  4.72378154,\n",
       "        6.45022446,  3.88230347,  9.1917795 , 12.15262677, 14.50707116,\n",
       "       10.22228434, 14.88584664,  5.85969563,  4.88985268,  9.8333569 ,\n",
       "       11.15219635,  9.90191576,  3.7976737 ,  9.73263988,  6.0865915 ,\n",
       "       12.00901765, 11.09979474,  7.18036064,  8.45839925,  6.82235943,\n",
       "        9.81196271, 12.86542696,  7.04282186, 11.51213955,  8.40922714,\n",
       "        7.6213815 ,  9.67890892,  6.89427303,  8.33905208,  6.40636632,\n",
       "       15.8941754 , 10.10579066,  7.90082348, 10.64193973,  9.66301585,\n",
       "        9.3370912 , 11.8425001 , 12.27252313,  8.40849656,  8.27254528,\n",
       "        9.17484491,  3.09423651,  5.45442681, 14.1006228 , 14.93490314,\n",
       "        9.25289188, 11.72967089, 10.93375046, 19.23664243, 13.35872473,\n",
       "        9.61624723,  7.13337868,  5.18066104, 10.61039091,  7.73094776,\n",
       "        5.73323887,  8.06028135,  6.75535599, 15.06142491, 12.64491927,\n",
       "        9.97608208, 14.43983242, 10.23210492,  7.4161474 , 14.56937223,\n",
       "       11.61673013,  6.88826154,  9.42898397,  7.37314524,  5.85160081,\n",
       "       12.77853264, 15.72824992,  5.80429728, 11.68890771,  8.04807229,\n",
       "        8.53862385,  8.22281823,  7.40802769, 10.14556488,  7.50714965,\n",
       "       10.81137048,  9.84928567,  9.28315586,  7.27730901,  8.26968601,\n",
       "       12.26617368, 11.50275156,  7.06733427, 10.29799692, 12.25416137,\n",
       "        4.99178416, 11.63008058,  8.01212872, 11.71179601,  7.71022253,\n",
       "        4.5853537 ,  5.11737269, 10.14425484, 10.77916751,  7.28705012,\n",
       "       11.91577738,  5.01543981,  9.8017606 ,  6.3669514 ,  8.04449168,\n",
       "       10.14219601,  7.4187599 ,  8.84633337, 13.01887843,  8.26932439,\n",
       "       12.50707634,  6.61087944, 11.58941253, 14.32470586,  2.5850665 ,\n",
       "        7.60931423, 11.73121638,  9.39086384, 11.11343762,  8.18804444,\n",
       "       10.25976936,  9.53296829, 13.50334618, 10.76326253, 11.01280799,\n",
       "        8.7643691 ,  8.53718133,  8.70232544, 11.18335643,  8.73704656,\n",
       "       10.86932457, 16.2262024 , 12.61337411,  9.0219294 , 13.60364177,\n",
       "        8.77577388,  3.88562639,  6.97574107,  4.38762424,  8.94545955,\n",
       "       10.05525514, 15.02931194, 10.98078212,  9.34269841, 12.48821674,\n",
       "        3.36659407, 10.70684367, 12.31259558,  5.56424126, 13.43126213,\n",
       "       11.01548922,  8.75413626, 11.8983456 , 16.81207857, 10.54559877,\n",
       "       10.74466176,  8.6219173 ,  7.45046689, 12.49100745,  7.43174852,\n",
       "       10.21469871,  8.56702766, 11.43693948, 11.00098632, 13.11261983,\n",
       "        8.4699508 ,  9.19037519,  7.06370885,  8.66712022, 11.13190148,\n",
       "       12.27096585,  7.23350403, 12.60881776, 14.06691358, 11.24030471,\n",
       "       15.63038744,  7.6786324 ,  6.26603589,  4.66383925, 14.48813293,\n",
       "       11.96309697,  9.83324599, 10.83990588,  6.62353286, 17.33725594,\n",
       "       10.38766355, 10.32818438, 12.17729987, 11.4430277 , 10.67165207,\n",
       "        7.62857663, 11.41440507, 15.64607349, 14.03626014, 14.77955988,\n",
       "        8.46635297,  7.03118554,  9.62263924, 10.16717474, 13.28257456,\n",
       "        4.92260611, 14.58865096,  9.5259763 ,  8.71935679,  6.96368687,\n",
       "        5.03542998, 12.46951175, 10.2199539 ,  6.1301173 ,  6.11476368,\n",
       "        8.9926459 , 15.00706458,  9.22122595,  5.49057114,  9.26277081,\n",
       "        9.18182929,  1.90934007,  9.8371154 ,  9.30719641, 12.08861909,\n",
       "       15.54686828, 13.37969509,  9.19333393,  6.68042227, 17.72007941,\n",
       "       10.1776553 , 10.04178788,  9.92762474, 10.59425428,  9.56691876,\n",
       "        8.27901398,  8.35942318,  9.90174019,  8.36972569,  7.86146265,\n",
       "       10.31929068,  9.23506835, 14.51197897,  2.04709057, 13.27452056,\n",
       "       13.73825558,  3.7798293 ,  8.97193722,  8.8856774 ,  5.77746492,\n",
       "        7.66654994,  6.66827246, 15.25681133, 12.80703518, 13.81466528,\n",
       "       12.16501619,  6.61284469,  8.4264392 , 11.46812368,  6.33361657,\n",
       "       12.13899529,  9.27902381,  8.87553758, 12.1328799 , 11.33278993,\n",
       "        8.9171015 , 13.47798941,  6.75681002, 11.84780682, 11.77930377,\n",
       "        9.07136068, 10.97839907,  6.24665927, 12.77208106,  9.44529359,\n",
       "        8.43183094, 13.14702768,  7.88696893,  5.77461611,  5.33011248,\n",
       "       11.81802985,  6.15871194, 15.26438255,  3.75421178, 15.0893691 ,\n",
       "       10.6330524 ,  9.70986066,  8.36524274, 11.19740834,  9.88709589,\n",
       "       13.30990565, 10.34268295, 10.45090528,  8.90916336,  9.82916313,\n",
       "       10.92340531,  4.86949482,  5.95544373, 12.22979228, 10.51259631,\n",
       "        9.44804999, 10.0553018 , 11.04274512,  8.38072096,  7.66508582,\n",
       "       10.58753577,  7.06488167, 11.22475827,  4.89224919, 13.08746691,\n",
       "       11.41779245, 10.7680892 , 12.94807295, 14.99642333, 13.0431102 ,\n",
       "        4.47737731,  6.1612691 ,  8.12554427, 10.07827315, 11.55297706,\n",
       "        7.82276856, 10.56030029,  7.7338512 ,  8.16544659,  5.78001671,\n",
       "        7.23030026,  5.94494618,  7.07238024, 13.16092539,  7.15180333,\n",
       "       17.89714619, 11.4799537 , 10.55450837,  7.42492666, 12.10092964,\n",
       "        8.27308652, 10.36602944, 17.68025361,  9.7118203 , 13.44781998,\n",
       "        7.89047072,  9.89503453, 15.31240191,  8.11909883, 15.43734567,\n",
       "       12.12325581,  8.31259967, 11.89722322, 12.91766335, 11.86542989,\n",
       "        5.28932584,  7.81858847,  9.25744409,  9.77669971, 11.86201629,\n",
       "       10.533103  ,  5.99396692, 11.14059355, 11.83175724, 11.67937134,\n",
       "       13.24234218, 12.50176646, 11.37754024,  9.78950287,  5.0171172 ,\n",
       "       11.28885466, 10.62306306, 10.81473651,  6.16975427,  6.75683038,\n",
       "       13.15945856,  9.88133454, 12.04450209, 10.08495513, 10.08926842,\n",
       "       12.81485142,  8.45186582, 10.28836233,  8.61317413,  8.69651132,\n",
       "        9.07248363, 10.66640131,  8.56375414, 13.76726838,  7.31617809,\n",
       "        9.43938507,  8.68080683, 14.34093365, 10.58966433, 13.09553362,\n",
       "        5.54331888, 10.8011508 , 12.66889239, 10.24685197, 13.19644113,\n",
       "        8.44813465, 14.22804232, 16.89669437,  8.91148432,  8.66349244,\n",
       "       14.36015343, 14.73871644,  8.43141992,  8.73943955,  9.15464617,\n",
       "        5.96664847,  7.24404416,  6.9875777 ,  7.6966073 ,  9.89594534,\n",
       "       10.7026442 , 14.65150148,  7.00493788, 12.9529672 ,  9.35803347,\n",
       "        9.85160887, 12.02445848,  6.63183394, 11.14722924, 10.49935662,\n",
       "       11.47735379, 10.86750593, 17.36590042,  8.08678005,  8.40700913,\n",
       "        8.13057842,  8.33356864,  8.08783862, 13.56704959, 14.26151274,\n",
       "        8.28776112,  7.50293328, 11.41424667,  8.34333087, 11.89879545,\n",
       "       10.60876906,  5.45276766, 14.6425156 , 15.38763302,  8.16163393,\n",
       "        8.83689532, 10.85759617, 11.00337037, 11.97563282, 16.03061362,\n",
       "        9.46915832,  7.60510827,  5.86204232,  7.80720988,  9.90061908,\n",
       "       15.38367359,  8.4471661 , 10.67136385,  9.95073131, 13.56517982,\n",
       "       17.58079728,  8.40739368,  8.53168167, 13.13248263, 12.04567447,\n",
       "       15.54012198, 11.75178456,  8.92212373, 11.77196449, 13.32611074,\n",
       "       12.46144654, 11.52182209, 13.20002407, 13.50788677, 14.14647697,\n",
       "       11.94612966,  9.49864576, 10.44014106, 13.6195269 ,  7.54919299,\n",
       "       11.10601993,  8.81998356, 10.08623447, 13.83535559, 10.5732972 ,\n",
       "       10.13930964,  5.92043158, 12.2387607 , 11.93645254, 16.48976417,\n",
       "        9.0766653 , 10.65745098, 10.74815105, 14.73235984,  9.7141134 ,\n",
       "       10.83706458, 11.82368953, 10.55982737,  8.66069916, 10.58226998,\n",
       "       13.22089525,  6.9204541 , 10.39890902,  7.89963756, 13.58513989,\n",
       "        5.43043929,  8.32323446, 11.13163563, 14.69657209,  9.80274922,\n",
       "        8.33440142, 15.64347121,  5.6559583 ,  3.40358213, 11.32004335,\n",
       "        8.49383733,  6.93630155, 12.12506934, 10.73140214,  8.30776411,\n",
       "        6.1590868 , 12.61737198, 11.95060353,  9.70247241, 15.53991099,\n",
       "        6.7897457 ,  5.42342449,  7.92427579,  9.86324195, 10.73001835,\n",
       "        9.27629183, 11.05616619,  6.24538173, 14.33129381,  9.75354646,\n",
       "       13.35188749, 11.02817604, 11.37025966, 11.70930184, 11.34312568,\n",
       "       11.92816828, 13.98745759, 10.58956351, 12.12701127,  9.73079292,\n",
       "       14.32035165,  7.97082309, 15.4028213 ,  9.87952615,  5.70767469,\n",
       "       10.38431324,  7.95684503, 12.52193065,  8.04212806,  8.6614497 ,\n",
       "        4.33137781,  8.64308104,  2.72836202,  5.24829153, 12.28124397,\n",
       "       12.35740048, 11.27637269,  7.09907157,  9.85686593,  9.98919238,\n",
       "        6.52490593, 14.51019491, 12.63208687,  9.33710748, 10.08065752,\n",
       "       10.62514842,  3.87479539,  9.25846785,  7.95404726,  6.99513997,\n",
       "        9.15669912, 15.39305958, 11.92252858,  8.28646303, 11.71774834])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norm(loc=10, scale=3).rvs(size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from scipy.stats._distn_infrastructure import rv_continuous\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class SelectAboveNoise(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # não é sklearn API friendly, mas acho útil aqui\n",
    "    def __init__(self, sampler:Type[rv_continuous], base_estimator=None, random_state=None):\n",
    "        self.sampler = sampler\n",
    "        self.base_estimator = base_estimator\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _validate_estimator(self, default):\n",
    "        if self.base_estimator is not None:\n",
    "            self.base_estimator_ = self.base_estimator\n",
    "        else:\n",
    "            self.base_estimator_ = default\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = self._validate_data(\n",
    "            X,\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "        \n",
    "        self._validate_estimator(RandomForestClassifier(random_state=self.random_state))\n",
    "        \n",
    "        self.base_estimator_.fit(X, y)\n",
    "        \n",
    "        self._selected_index_list_ = \\\n",
    "        (pd.DataFrame(list(zip(range(self.n_features_in_), self.base_estimator_.feature_importances_)),\n",
    "                      columns=['feature_name', 'feature_importance'])\n",
    "         .sort_values(by='feature_importance', ascending=False)\n",
    "         .head(self.K)\n",
    "         .index\n",
    "         .to_list()\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "        return np.array([feat in self._selected_index_list_ for feat in range(self.n_features_in_)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog_datalab_boruta2",
   "language": "python",
   "name": "blog_datalab_boruta2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
