{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST COMPLEMENTAR PARA O VITALISET.GITHUB.IO: ESTRATÉGIAS DE FEATURE SELECTION DO SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlo\\anaconda3\\envs\\blog_datalab_boruta2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boruta Feature Selection\n",
    "\n",
    "Fazer uma seleção de variáveis adequada nos ajuda, principalmente, por que ter menos atributos nos retorna um modelo menos complexo e, consequentemente, com menos chance de se apegar em relações particulares da amostra utilizada para treino. Na prática, isso significa, menos chance de overfitting e, consequentemente, nos dá modelos mais estáveis e com melhor performance fora do laboratório. Por isso, na minha opinião, a seleção de variáveis é uma das mais poderosas ferramentas data-centric.\n",
    "\n",
    "*$\\oint$ Para ilustrar a afirmação anterior, temos, como exemplo, que a [dimensão-VC](https://youtu.be/Dc0sr0kdBVI) (medida de complexidade de uma família de hipóteses) de um perceptron (classificador linear) é $d+1$ em que $d$ é a número de variáveis utilizadas no modelo.* \n",
    "\n",
    "Entretanto, esse assunto não é visto com o cuidado devido na maioria dos cursos de Aprendizado de Máquina. São apresentados poucos métodos e de maneira superficial. Os poucos lugares que discutem o tema, no geral, focam ainda em técnicas que não são escaláveis com o aumento de variáveis e por isso são impraticáveis na maioria das aplicações do mercado (como as estratégias gulosas de [`sklearn.feature_selection.SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)).\n",
    "\n",
    "No DataLab, seleção de variáveis se torna extremamente relevante pela natureza dos problemas que trabalhamos. Na grande maioria dos casos temos algumas milhares de variáveis disponíveis no bureau de dados da Serasa e não é fácil identificar de antemão quais serão as features que nos darão mais ganhos. É necessário aplicar técnicas que são robustas à grandeza de variáveis que temos ao mesmo tempo que garantem uma seleção que faça sentido.\n",
    "\n",
    "Neste post iremos motivar a construção do Boruta, uma das técnicas mais utilizadas pelos cientistas do DataLab na seleção de features, com algumas dicas de uso prático. Ilustraremos ainda o uso da função [`boruta.BorutaPy`](https://github.com/scikit-learn-contrib/boruta_py), do ambiente [scikit-learn-contrib](https://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md) (ou seja, compátivel com bibliotecas que sigam o [padrão de código do scikit-learn](https://scikit-learn.org/stable/developers/develop.html).\n",
    "\n",
    "___\n",
    "\n",
    "Para ilustrar o problema de seleção de features, utilizaremos o [`sklearn.datasets.make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) para criar um problema genérico de classificação em que podemos definir, como um parâmetro da função, o número de variáveis úteis para o problema de previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>...</th>\n",
       "      <th>column_17</th>\n",
       "      <th>column_18</th>\n",
       "      <th>column_19</th>\n",
       "      <th>column_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.050478</td>\n",
       "      <td>-1.323568</td>\n",
       "      <td>0.912474</td>\n",
       "      <td>1.009796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800511</td>\n",
       "      <td>1.238946</td>\n",
       "      <td>0.209659</td>\n",
       "      <td>-0.491636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.580834</td>\n",
       "      <td>-2.747104</td>\n",
       "      <td>1.777419</td>\n",
       "      <td>1.850430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524088</td>\n",
       "      <td>0.152355</td>\n",
       "      <td>-0.822420</td>\n",
       "      <td>1.121031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.885704</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>0.631813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262561</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>-0.137372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.525438</td>\n",
       "      <td>-2.967793</td>\n",
       "      <td>1.884777</td>\n",
       "      <td>1.924410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617652</td>\n",
       "      <td>-0.316073</td>\n",
       "      <td>0.615771</td>\n",
       "      <td>1.203884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.076826</td>\n",
       "      <td>-1.014619</td>\n",
       "      <td>0.752233</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326745</td>\n",
       "      <td>0.300474</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>-1.138833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   column_1  column_2  column_3  column_4  ...  column_17  column_18  \\\n",
       "0 -1.050478 -1.323568  0.912474  1.009796  ...   1.800511   1.238946   \n",
       "1 -1.580834 -2.747104  1.777419  1.850430  ...  -0.524088   0.152355   \n",
       "2 -0.885704 -0.614600  0.501004  0.631813  ...   0.262561   0.193590   \n",
       "3 -1.525438 -2.967793  1.884777  1.924410  ...  -0.617652  -0.316073   \n",
       "4 -1.076826 -1.014619  0.752233  0.885267  ...   0.326745   0.300474   \n",
       "\n",
       "   column_19  column_20  \n",
       "0   0.209659  -0.491636  \n",
       "1  -0.822420   1.121031  \n",
       "2   0.850898  -0.137372  \n",
       "3   0.615771   1.203884  \n",
       "4   0.622207  -1.138833  \n",
       "\n",
       "[5 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "N_FEATURES = 20\n",
    "\n",
    "X, y = \\\n",
    "make_classification(n_samples=1000,\n",
    "                    n_features=N_FEATURES,\n",
    "                    n_informative=2,\n",
    "                    n_redundant=2,\n",
    "                    n_classes=2,\n",
    "                    flip_y=0.1,\n",
    "                    shuffle=False,\n",
    "                    random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f'column_{i+1}' for i in range(N_FEATURES)])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos escolhendo 2 features informativas e 2 features redundantes, temos que as 4 features mais importantes são as colunas: column_1, column_2, column_3 e column_4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção do Boruta\n",
    "\n",
    "## Medindo a importância de uma variável\n",
    "\n",
    "Uma das técnicas mais comuns para selecionar as variáveis é aproveitar-se de modelos que, de alguma forma, selecionam as variáveis no próprio processo de treinamento. Árvores e, consequentemente, cômites de árvores são talvez o melhor exemplo disso: pela [estratégia gulosa de fazer sempre a melhor quebra possível naquele instante](https://www.edureka.co/community/46109/what-is-greedy-approach-in-decision-tree-algorithm) (de acordo com algum critério de melhor, usualmente relacionada a pureza das folhas criadas, no caso de classificação), estamos sempre escolhendo a melhor quebra da melhor variável para ser feita naquela etapa. Variáveis pouco discriminativas são escolhidas muito menos que as variáveis que de fato ajudam a fazer a previsão.\n",
    "\n",
    "Esse processo naturalmente deriva medidas de importâncias para as variáveis como: o número de vezes que ela é utilizada (esse é o modo default do atributo `.feature_importance_` dos ensembles do LGBM, como o [`lightgbm.LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html)) ou uma ponderação do ganho de critério no processo de escolha das quebras das features (essa é a forma default dos ensembles de árvores do sklearn, como o [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), o [`sklearn.ensemble.ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html), e o [`sklearn.ensemble.HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) e também vira o atributo do LGBM quando setamos o `importance_type='gain'`).\n",
    "\n",
    "Vale comentar brevemente que no final, esses valores são normalizados de tal forma que `.feature_importances_.sum()` = 1.\n",
    "\n",
    "Com alguma dessas medidas naturais de importância, é razoável ordenar nossas variáveis da mais importante para a menos importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.feature_importances_.sum()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.278748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.201150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.092612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.018641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_18</td>\n",
       "      <td>0.017565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.016912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  feature_importance\n",
       "0      column_2            0.278748\n",
       "1      column_3            0.201150\n",
       "2      column_4            0.092612\n",
       "..          ...                 ...\n",
       "17    column_16            0.018641\n",
       "18    column_18            0.017565\n",
       "19    column_20            0.016912\n",
       "\n",
       "[20 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42).fit(X, y)\n",
    "\n",
    "df_imp = \\\n",
    "(pd.DataFrame(list(zip(X.columns, rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$\\oint$ Existem algumas outras formas de metrificar a importância de uma variável, como por exemplo utilizando suas contribuições de [valores SHAP](https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30). Como o [`shap.Explainer(model).shap_values(X)`](https://github.com/slundberg/shap) nos retorna uma medida de quanto aquela variável agregou na previsão, pegar a sua média entre todos os exemplos nos dá uma forma de ver o quão útil ela foi para discriminar os exemplos como um todo. Para os valores não se cancelarem (imagine uma variável que para determinados valores joga a previsão para cima e em outros valores joga a previsão para baixo), tomamos o módulo antes de fazer a média. Repare que a ordem das importâncias dada pelo SHAP pode ser diferente da ordem de importâncias dada pelo atributo de `.feature_importance_` usual do estimador, como é o caso do nosso exemplo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>shap_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.197645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.107211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.043797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>column_16</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_5</td>\n",
       "      <td>0.005099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  shap_importance\n",
       "0      column_2         0.197645\n",
       "1      column_3         0.107211\n",
       "2      column_4         0.043797\n",
       "..          ...              ...\n",
       "17    column_16         0.005268\n",
       "18     column_5         0.005099\n",
       "19    column_20         0.005019\n",
       "\n",
       "[20 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(rfc)\n",
    "shap_vals = explainer.shap_values(X)\n",
    "\n",
    "df_imp_shap = \\\n",
    "(pd.DataFrame(list(zip(X.columns, np.abs(shap_vals[0]).mean(axis=0))),\n",
    "              columns=['feature_name', 'shap_importance'])\n",
    " .sort_values(by='shap_importance', ascending=False)\n",
    " .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_imp_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ainda não falamos do Boruta, mas ele se utiliza dessa ordenação para fazer suas análises e é implementado, usualmente utilizando medida de importância do estimador (o atributo `.feature_importances_` ou `.coef_` para algoritmos lineares). Essa diferença motivou alguns contribuidores a implementar o [Boruta-Shap](https://github.com/Ekeany/Boruta-Shap). Infelizmente a biblioteca não é tão bem estruturada quanto a do Boruta. Minha sugestão é adaptar na mão o atributo `.feature_importance_` do seu classificador, fazendo algo como:*\n",
    "\n",
    "```python\n",
    "class SHAPImportanceRandomForestClassifier(RandomForestClassifier):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.X_ = X\n",
    "        super().fit(X, y, sample_weight=sample_weight)\n",
    "        return self\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        check_is_fitted(self)\n",
    "        explainer = shap.TreeExplainer(self)\n",
    "        shap_vals = explainer.shap_values(self.X_)\n",
    "        return np.abs(shap_vals[0]).mean(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04156985, 0.19764501, 0.10721142, 0.04379691, 0.00509938,\n",
       "       0.00967927, 0.00900892, 0.00769202, 0.01053711, 0.00973848,\n",
       "       0.00764462, 0.00725161, 0.00690175, 0.00718789, 0.00600269,\n",
       "       0.00526766, 0.00659648, 0.00585107, 0.00726538, 0.00501896])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shap_feature_importances_ import SHAPImportanceRandomForestClassifier\n",
    "\n",
    "rfc_shap = SHAPImportanceRandomForestClassifier(random_state=42).fit(X, y)\n",
    "rfc_shap.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note que essa implementação utiliza o mesmo conjunto de treino para cálculo do SHAP. Existe algum debate aqui, mas tenha em mente que os valores de importância calculados com SHAP (média do valor absoluto) no teste podem ser diferentes dos valores de importância calculados com SHAP no treino. Se você quiser esse nível de preciosismo, pode estar interessado em reservar um pedaço do seu conjunto de dados para calcular os valores SHAP. Implemento essa ideia na classe `XSHAPImportanceRandomForestClassifier` do arquivo [`shap_feature_importances_.py`](link?) no [repositório desse post](link?). Para poder dormir tranquilo, tenha em mente que o `.feature_importances_` usual dos algoritmos baseados em árvore é calculado com o conjunto de treino, então calcular o SHAP no treino não é uma blasfêmia tão grande.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionando as `K` melhores\n",
    "\n",
    "Se queremos que nosso modelo tenha apenas as `K` features mais úteis, a maneira natural de escolher elas seria pegar as `K` variáveis com maiores valores de importância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column_2', 'column_3', 'column_4', 'column_1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 4\n",
    "\n",
    "(df_imp\n",
    " .head(K)\n",
    " .feature_name\n",
    " .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma das estratégias mais comuns de se fazer seleção de features no mercado, mas levanta algumas questões. A primeira e mais imediata é: como escolher o número de variáveis `K` ideal. Nesse caso ilustrativo, sabemos que 4 variáveis é o número correto, mas na maioria dos casos de aplicação real é irrealista ter esse número de antemão.\n",
    "\n",
    "*$\\oint$ Uma estratégia muito utilizada, mas que não vamos focar muito, é aumentar a lista de features do modelo seguindo a ordenação dada pelo modelo treinado em todas as features. Encarando esse valor `K` como um hiper-parâmetro que estamos otimizando. No exemplo abaixo, fazemos isso de utilizando o [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ao construir uma classe `SelectKTop` utilizando o padrão necessário para os selecionadores de variáveis do scikit-learn, isto é, seguindo a forma que o [`sklearn.feature_selection.SelectorMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectorMixin.html) exige. Você pode ver a implementação dessa classe no arquivo [`selectktop_selector.py`](link?) no [repositório desse post](link?).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selectktop_selector import SelectKTop\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "grid = \\\n",
    "(GridSearchCV(make_pipeline(SelectKTop(random_state=42), \n",
    "                            RandomForestClassifier(random_state=42)),\n",
    "              param_grid={'selectktop__K': np.arange(1,N_FEATURES+1)},\n",
    "              cv= RepeatedStratifiedKFold(n_splits=3, \n",
    "                                          n_repeats=1, \n",
    "                                          random_state=42),\n",
    "              scoring='roc_auc',\n",
    "#               verbose=3\n",
    "             )\n",
    " .fit(X, y)\n",
    ")\n",
    "\n",
    "df_cv = \\\n",
    "(pd.DataFrame(grid.cv_results_)[['param_selectktop__K', \n",
    "                                 'mean_test_score', \n",
    "                                 'std_test_score']])\n",
    "\n",
    "cv_best = \\\n",
    "(df_cv.\n",
    " sort_values(by='mean_test_score', ascending=False)\n",
    " .reset_index(drop=True)\n",
    " .loc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(df_cv.param_selectktop__K, df_cv.mean_test_score, 1.96*df_cv.std_test_score)\n",
    "plt.scatter(cv_best.param_selectktop__K, cv_best.mean_test_score, s=100)\n",
    "plt.ylim(0.75, 1)\n",
    "plt.xlabel('Número de features selecionadas')\n",
    "plt.xticks(df_cv.param_selectktop__K.astype(int))\n",
    "plt.ylabel('Performance (ROCAUC)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No nosso experimento controlado, encontramos algumas poucas variáveis a mais do que o correto.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_1', 'column_2', 'column_3', 'column_4', 'column_6',\n",
       "       'column_10'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vale citar que esse método pode ser deixado mais robusto variando o `random_seed` do `base_estimator` e tendo uma distribuição de importâncias para cada variável ao invés de apenas um valor único (que naturalmente é mais ruídoso). Utilizar essa técnica, mas com o SHAP para medir a importância (passando por exemplo o `SHAPImportanceRandomForestClassifier` como `base_estimator` do `SelectKTop`) é uma técnica muito utilizada por alguns cientistas do DataLab como alternativa ao Boruta, que costuma ser mais demorado.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionando as `K` melhores incluindo uma variável aleatória\n",
    "\n",
    "Criar uma variável de ruído, ou seja, que sabidamente não é útil para a previsão, nos auxiliar a ter um ponto de corte para filtro das variáveis que demonstram ajudar na previsão. A ideia dessa abordagem é medir a importância da variável aleatória e ficar apenas com variáveis que se demonstraram mais importantes.\n",
    "\n",
    "Adicionando a nova coluna, por exemplo, amostrada de uma variável aleatória $\\mathcal{N}(0,1)$ de forma independente, temos uma nova lista de importância das variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_column</th>\n",
       "      <th>column_20</th>\n",
       "      <th>column_19</th>\n",
       "      <th>column_18</th>\n",
       "      <th>...</th>\n",
       "      <th>column_4</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.491636</td>\n",
       "      <td>0.209659</td>\n",
       "      <td>1.238946</td>\n",
       "      <td>...</td>\n",
       "      <td>1.009796</td>\n",
       "      <td>0.912474</td>\n",
       "      <td>-1.323568</td>\n",
       "      <td>-1.050478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.138264</td>\n",
       "      <td>1.121031</td>\n",
       "      <td>-0.822420</td>\n",
       "      <td>0.152355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.850430</td>\n",
       "      <td>1.777419</td>\n",
       "      <td>-2.747104</td>\n",
       "      <td>-1.580834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>-0.137372</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631813</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>-0.885704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.523030</td>\n",
       "      <td>1.203884</td>\n",
       "      <td>0.615771</td>\n",
       "      <td>-0.316073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.924410</td>\n",
       "      <td>1.884777</td>\n",
       "      <td>-2.967793</td>\n",
       "      <td>-1.525438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-1.138833</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.300474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>0.752233</td>\n",
       "      <td>-1.014619</td>\n",
       "      <td>-1.076826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   noise_column  column_20  column_19  column_18  ...  column_4  column_3  \\\n",
       "0      0.496714  -0.491636   0.209659   1.238946  ...  1.009796  0.912474   \n",
       "1     -0.138264   1.121031  -0.822420   0.152355  ...  1.850430  1.777419   \n",
       "2      0.647689  -0.137372   0.850898   0.193590  ...  0.631813  0.501004   \n",
       "3      1.523030   1.203884   0.615771  -0.316073  ...  1.924410  1.884777   \n",
       "4     -0.234153  -1.138833   0.622207   0.300474  ...  0.885267  0.752233   \n",
       "\n",
       "   column_2  column_1  \n",
       "0 -1.323568 -1.050478  \n",
       "1 -2.747104 -1.580834  \n",
       "2 -0.614600 -0.885704  \n",
       "3 -2.967793 -1.525438  \n",
       "4 -1.014619 -1.076826  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_X = (X.assign(noise_column = np.random.RandomState(42).normal(size=X.shape[0])))\n",
    "noised_X[noised_X.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_2</td>\n",
       "      <td>0.266446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column_3</td>\n",
       "      <td>0.205667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>column_4</td>\n",
       "      <td>0.087548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>column_5</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>column_19</td>\n",
       "      <td>0.018264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>column_20</td>\n",
       "      <td>0.017692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  feature_importance\n",
       "1      column_2            0.266446\n",
       "2      column_3            0.205667\n",
       "3      column_4            0.087548\n",
       "..          ...                 ...\n",
       "4      column_5            0.018706\n",
       "18    column_19            0.018264\n",
       "19    column_20            0.017692\n",
       "\n",
       "[21 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_rfc = RandomForestClassifier(random_state=42).fit(noised_X, y)\n",
    "\n",
    "df_imp_normal_noise = \\\n",
    "(pd.DataFrame(list(zip(noised_X.columns, noised_rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    ")\n",
    "\n",
    "df_imp_normal_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a última variável é a nossa coluna sabidamente ruídosa, a ideia dessa técnica é selecionar apenas as variáveis que tem importância maior do que o limiar settado pela importância da variável não relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_2', 'column_3', 'column_4', 'column_1', 'column_6',\n",
       "       'column_10', 'column_14'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(\n",
    " df_imp_normal_noise\n",
    " .query(f\"feature_importance > {noised_rfc.feature_importances_[-1]}\")\n",
    " .feature_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale observar que a escolha da variável ruídosa como $\\mathcal{N}(0,1)$ foi totalmente arbitrária. Entretanto isso faz diferença e pode fazer a seleção de variáveis ser diferente. No nosso exemplo controlado, mudar o ruído para $\\textrm{Exp}(1)$ nos faria selecionar variáveis finais diferentes, totalmente, por sorte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['column_2', 'column_3', 'column_4', 'column_1', 'column_14',\n",
       "       'column_6', 'column_10', 'column_9'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised2_X = (X.assign(noise_column = np.random.RandomState(42).exponential(size=X.shape[0])))\n",
    "noised2_rfc = RandomForestClassifier(random_state=0).fit(noised2_X, y)\n",
    "\n",
    "np.array(\n",
    " pd.DataFrame(list(zip(noised2_X.columns, noised2_rfc.feature_importances_)),\n",
    "              columns=['feature_name', 'feature_importance'])\n",
    " .sort_values(by='feature_importance', ascending=False)\n",
    " .query(f\"feature_importance > {noised_rfc.feature_importances_[-1]}\")\n",
    " .feature_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso nos demonstra um problema desse método. Apesar de poderoso, por nos dar um jeito interessante de selecionar as variáveis sem escolher `K` de forma arbitrária, a escolha da distribuição da variável ruídosa é uma fonte de variação relevante.\n",
    "\n",
    "Em muitos casos, ter variáveis discretas *versus* contínuas pode influenciar na medida de importância (como é o caso de árvores que, por ter mais quebras disponíveis, terão mais chance de escolher uma variável ruídosa contínua) ou a própria escala da feature adicionada pode atrapalhar nessa mensuração (por exemplo, por estarmos usando os coefiencientes angulares de um [`sklearn.linear_model.Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)).\n",
    "\n",
    "Toda essa variabilidade pode fazer uma *feature* ruim ser selecionada as vezes e em outras vezes uma variável boa ser discartada simplemente por azar.\n",
    "\n",
    "O Boruta vem para tentar lidar com essas duas questões ao mesmo tempo: tentar manter a distribuição marginal das features ruídosas iguais às distribuições marginais das features originais, enquanto tenta ser robusto à variabilidade, repetindo o experimento algumas vezes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideias gerais do Boruta\n",
    "\n",
    "Já existem muitos textos úteis que explicam o Boruta de forma didática e com exemplos. Como a ideia desse post não é ser redundante com a literatura e sim compilar ideias centrais de uso prático, vamos apenas citar os principais aspectos e deixar o convite para uma leitura detalhada de outras referências do tema como o post [Boruta Explained Exactly How You Wished Someone Explained to You](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a). A construção que fizemos anteriormente vão deixar as ideias do Boruta ainda mais claras, justificando o modo de serem.\n",
    "\n",
    "Em resumo, o Boruta:\n",
    "- Cria variáveis não correlacionadas com a target ao embaralhar, entre as linhas, variáveis já presentes no dataset. Essas são as variáveis que chamamos de *shadow*.\n",
    "- Lida com a variabilidade repetindo o processo várias vezes e marcando quantas vezes a nossa variável de interesse ficou atrás do percentil `perc` dos `.feature_importances` das *shadow features* (por default, `perc=100`, e portanto comparamos com o máximo dos `.feature_importances` das *shadow features*, isto é, se alguma *shadow* for melhor, já descartamos aquela variável de interesse naquela rodada).\n",
    "- Por fim, um teste de hipótese é feito para avaliar se podemos afirmar com alguma significância estatítica `alpha` que a feature de interesse é melhor que o percentil `perc` da importância das *shadow features*.\n",
    "- Do teste de hipótese ele divide o conjunto de features em três categorias:\n",
    "    - Aquelas que \"com certeza\" são melhores que os ruídos introduzidos pelas *shadow features* (`.support_`);\n",
    "    - Aquelas que \"com certeza\" não são melhores que com certeza são equivalentes às variáveis *shadow*;\n",
    "    - Aquelas que não é possível afirmar com significância estatística (`.weak_support_`).\n",
    "- Na prática, a partir do momento que ele tem confiança que uma determinada variável não é importante, ele já exclui ela das próximas iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso do Boruta\n",
    "\n",
    "Primeiro, precisamos instanciar um `base_estimator` que será utilizado dentro do Boruta para calcular a importância das variáveis (através do `.feature_importances_` ou do `.coef_`). É importante ressaltar que podemos adicionar hiper-parâmetros que acharmos relevantes para o problema, com o `class_weight` para o problema desbalanceado.\n",
    "\n",
    "Quando usamos cômites de árvores (sejamos honestos, sempre), é importante ter em mente que árvores profundas vão mudar o `.feature_importances_`, mas vão demorar mais para treinar. É razoável utilizar árvores mais rasas, uma vez que os ganhos mais expressivos são feitos nas primeiras quebras usualmente. Como rodaremos o algoritmo mais rápido, podemos aumentar o número de rodadas que o Boruta vai rodar por isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "boruta_forest = RandomForestClassifier(max_depth=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ponto de atenção importante que não é necessariamente claro na documentação, é que o parâmetro `n_estimators` do Boruta sobrescreve o `n_estimators` do estimador como podemos ver no [código fonte do BorutaPy](https://github.com/scikit-learn-contrib/boruta_py/blob/0.3/boruta/boruta_py.py#L268):\n",
    "```python\n",
    "# set n_estimators\n",
    "if self.n_estimators != 'auto':\n",
    "    self.estimator.set_params(n_estimators=self.n_estimators)\n",
    "```\n",
    "Por default, `n_estimators=1000`. Se `n_estimators='auto'`, então [uma regra baseada no número de features que estamos avaliando é feita para escolher o número de árvores do cômite](https://github.com/scikit-learn-contrib/boruta_py/blob/0.3/boruta/boruta_py.py#L371)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, `alpha` e `perc` são os outros parâmetros importantes do [`boruta.BorutaPy`](https://github.com/scikit-learn-contrib/boruta_py) que você deveria ficar atento:\n",
    "- O `perc` (percentil do `.feature_importances_` das *shadow features* utilizado para decidir se as variáveis foram boas ou não naquela determinada rodada) é um `int` que vai de 0 a 100. Quanto mais próximo de 100, mais rigoroso estamos sendo na hora de avaliar nossas features. Por sorte, alguns `.feature_importances_` de *shadow features* podem ser grandes e ser muito rigoroso com o critério de corte pode ser ruim porque estaremos excluindo variáveis marginais que são relevantes, mas não tem uma importância tão expressiva. O default desse parâmetro é 100.\n",
    "- O `alpha` é um float que vai de 0 até 1 e será importante para delimitar a partição que fazemos do conjunto de variáveis (`.support_weak_`, `.support` e excluídas) uma vez que determinará o rigor de certeza que queremos ter para afirmar que uma determinada feature é relevante ou não para o problema de classificação (ou regressão). O default desse parâmetro é 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestClassifier(max_depth=10, n_estimators=1000,\n",
       "                                          random_state=RandomState(MT19937) at 0x23BF0CEFE40),\n",
       "         random_state=RandomState(MT19937) at 0x23BF0CEFE40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestClassifier(max_depth=10, n_estimators=1000,\n",
       "                                          random_state=RandomState(MT19937) at 0x23BF0CEFE40),\n",
       "         random_state=RandomState(MT19937) at 0x23BF0CEFE40)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=1000,\n",
       "                       random_state=RandomState(MT19937) at 0x23BF0CEFE40)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=1000,\n",
       "                       random_state=RandomState(MT19937) at 0x23BF0CEFE40)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BorutaPy(estimator=RandomForestClassifier(max_depth=10, n_estimators=1000,\n",
       "                                          random_state=RandomState(MT19937) at 0x23BF0CEFE40),\n",
       "         random_state=RandomState(MT19937) at 0x23BF0CEFE40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "boruta = BorutaPy(\n",
    "   estimator = boruta_forest,\n",
    "   max_iter = 100, # number of trials to perform\n",
    "   random_state = 42\n",
    ")\n",
    "\n",
    "### fit Boruta (it accepts np.array, not pd.DataFrame)\n",
    "boruta.fit(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, é fácil resgatar as features com os atributos `.support_` e `.support_weak_`, como já foi comentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['column_1', 'column_2', 'column_3', 'column_4', 'column_6', 'column_10'],\n",
       " ['column_9', 'column_14'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_area = X.columns[boruta.support_].to_list()\n",
    "blue_area = X.columns[boruta.support_weak_].to_list()\n",
    "\n",
    "green_area, blue_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False,  True, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boruta.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunnando o `perc`\n",
    "\n",
    "Os parâmetros usuais do [`boruta.BorutaPy`](https://github.com/scikit-learn-contrib/boruta_py) costumam funcionar muito bem quando temos um número de variáveis baixo. Entretanto, quando temos muitas *features*, criaremos muitas *shadow features* e portanto a restrição de `perc=100` pode ser forte demais.\n",
    "\n",
    "Faremos um pequeno experimento em que variaremos número de feature do nosso dataset e veremos quantas features relevantes estamos pegando, fixado um valor de `perc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colocar isso daqui tudo dentro de um .py \\/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [31:49<00:00, 212.18s/it]\n"
     ]
    }
   ],
   "source": [
    "val=[]\n",
    "\n",
    "for N_FEATURES_exp in tqdm(np.linspace(100, 500, 9).astype(int)):\n",
    "    X_exp, y_exp = \\\n",
    "    make_classification(n_samples=10000,\n",
    "                        n_features=N_FEATURES_exp,\n",
    "                        n_informative=int(0.2*N_FEATURES_exp),\n",
    "                        n_redundant=0,\n",
    "                        n_classes=2,\n",
    "                        flip_y=0.1,\n",
    "                        shuffle=False,\n",
    "                        random_state=42)\n",
    "    \n",
    "    X_exp = pd.DataFrame(X_exp, columns=[f'column_{i+1}' for i in range(N_FEATURES_exp)])\n",
    "    \n",
    "    boruta_exp = BorutaPy(\n",
    "        estimator = RandomForestClassifier(max_depth=7, random_state=42),\n",
    "        n_estimators=100,\n",
    "        max_iter = 100,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "    boruta_exp.fit(np.array(X_exp), np.array(y_exp))\n",
    "    \n",
    "    green_area_exp = X_exp.columns[boruta_exp.support_].to_list()\n",
    "    blue_area_exp = X_exp.columns[boruta_exp.support_weak_].to_list()\n",
    "    \n",
    "    # mexer nisso\n",
    "    val.append(len(set(green_area_exp+blue_area_exp)\n",
    "                   .intersection(set([f'column_{i+1}' for i in range(int(0.2*N_FEATURES_exp))]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar isso pra perc= 100, 90, 80, 70, 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, o Boruta demora razoavelmente bastante. Uma coisa que pode ajudar é fazer um undersample da base de desenvolvimento e filtrar as variáveis a partir apenas dessa amostra. Vamos fazer um último experimento em que, com os parametros do [`boruta.BorutaPy`](https://github.com/scikit-learn-contrib/boruta_py) estão fixados, mas mexemos no `frac` do `X.sample` e veremos se perdemos muito ao filtrar nosso conjunto de dados para ganhar tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO ^^^^ \n",
    "\n",
    "# MEDIR TEMPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IGNORAR DAQUI PRA BAIXO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Selector\n",
    "\n",
    "O primeiro algoritmo que iremos discutir é a técnica de seleção sequencial. A ideia aqui é utilizar uma estratégia gulosa para explorar o número de combinações possíveis de variáveis que temos disponível. A príncipio, temos `2**N_FEATURES` formas diferentes de escolher as `N_FEATURES` disponíveis. Construir todos esses `2**N_FEATURES` modelos é impraticavel. No nosso simples exemplo, com apenas 20 features, teríamos `1048576` combinações diferentes. Se cada modelo demorasse 1 segundo para ser treinado e avaliado, esperaríamos 12 dias até termos todos esses resultados. Com 5 features a mais, teríamos que esperar mais de 1 ano.\n",
    "\n",
    "A ideia da busca gulosa é escolher uma feature de cada vez para ser adicionada (ou removida) da lista de features selecionadas e observar a mudança na performance do modelo.\n",
    "\n",
    "Por exemplo, na direção \"para frente\", podemos comparar o modelo sem nenhuma variável (por exemplo um modelo que retorna alguma estatística dos `y_train`, como a moda, no caso de claissificação, ou uma média/mediana no caso de uma regressão) como todos os `N_FEATURES` modelos possíveis com 1 variável. A variável do modelo que tiver a melhor performance entre os `N_FEATURES` modelos de 1 variável é escolhida para ser a primeira variável selecionada. Para escolher a possível segunda variável, fazemos a mesma coisa comparando \n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs = SequentialFeatureSelector(estimator, direction = \"forward\", n_features_to_select=3)\n",
    "# other optionfor direction: \"backward\"\n",
    "sfs.fit(X, y)\n",
    "feature_mask = sfs.get_support()\n",
    "X_selected_features = sfs.transform(X)\n",
    "```\n",
    "\n",
    "This Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion (one feature at a time).\n",
    "\n",
    "Cons: Even with the greedy approach, it's expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Selector\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThresholdn\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(X)\n",
    "X_selected_features = selector.transform(X)\n",
    "```\n",
    "\n",
    "Feature selector that removes all low-variance features.\n",
    "\n",
    "Cons: which threshold to pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Best Selector\n",
    "\n",
    "SelectKBest is probably the most common technique. We simply select features according to the k highest scores (some measure of feature importances).\n",
    "\n",
    "For instance, you can take the most \"correlated\" features to the target:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "X_selected_features = SelectKBest(mutual_info_regression, k=20).fit_transform(X, y)\n",
    "\n",
    "```\n",
    "\n",
    "In practice, we normaly use it with some model measure of feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectorMixin pro variável ruído:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.49014246,  9.5852071 , 11.94306561, 14.56908957,  9.29753988,\n",
       "        9.29758913, 14.73763845, 12.30230419,  8.59157684, 11.62768013,\n",
       "        8.60974692,  8.60281074, 10.72588681,  4.26015927,  4.8252465 ,\n",
       "        8.31313741,  6.96150664, 10.942742  ,  7.27592777,  5.7630889 ,\n",
       "       14.39694631,  9.3226711 , 10.20258461,  5.72575544,  8.36685183,\n",
       "       10.33276777,  6.54701927, 11.12709406,  8.19808393,  9.12491875,\n",
       "        8.19488016, 15.55683455,  9.95950833,  6.82686721, 12.46763474,\n",
       "        6.33746905, 10.62659079,  4.12098963,  6.01544185, 10.59058371,\n",
       "       12.21539974, 10.51410484,  9.65305515,  9.09668891,  5.56443403,\n",
       "        7.84046737,  8.61808369, 13.17136668, 11.03085487,  4.71087953,\n",
       "       10.97225191,  8.84475316,  7.969234  , 11.83502887, 13.09299857,\n",
       "       12.79384036,  7.48234743,  9.07236287, 10.99379029, 12.92663538,\n",
       "        8.56247729,  9.44302307,  6.68099508,  6.41138013, 12.43757747,\n",
       "       14.06872009,  9.78396964, 13.01059869, 11.08490808,  8.06464074,\n",
       "       11.08418682, 14.6141097 ,  9.89252188, 14.69393097,  2.14076469,\n",
       "       12.46570751, 10.2611412 ,  9.10297795, 10.27528233,  4.03729326,\n",
       "        9.34098434, 11.07133771, 14.43368213,  8.44518935,  7.57451919,\n",
       "        8.49472887, 12.74620635, 10.98625333,  8.41071939, 11.5398023 ,\n",
       "       10.29123265, 12.90593497,  7.89384072,  9.01701356,  8.82367554,\n",
       "        5.60945516, 10.88836083, 10.78316582, 10.01534037,  9.2962386 ,\n",
       "        5.75388777,  8.73806403,  8.97185645,  7.59316819,  9.51614287,\n",
       "       11.21215257, 15.6585577 , 10.52373344, 10.77265117,  9.77666225,\n",
       "        4.24368635,  9.92045837, 10.18069063, 17.38972634,  9.42291711,\n",
       "       10.90464203,  9.89586469,  6.49396589, 13.42846844, 12.2557991 ,\n",
       "       12.37309584,  7.27183764, 14.20838293,  5.79444681, 11.76057128,\n",
       "       16.57136688,  7.02839102,  8.30110681, 10.2989541 ,  8.48957304,\n",
       "        5.34800971, 10.20568892,  6.81308886, 11.42077729,  7.2417273 ,\n",
       "       14.64980322,  7.65024012,  9.03381545, 12.44055165,  6.30740705,\n",
       "       10.6823798 , 13.92142826,  5.1775503 , 10.55390158, 10.77964838,\n",
       "       12.34546862,  6.28914787,  6.03863016, 11.5658247 , 10.89095402,\n",
       "       10.75147855, 11.03934463,  7.95992584, 10.69676109, 10.87921742,\n",
       "        7.85694575, 15.59732353, 11.42149876,  6.42608951, 11.96966083,\n",
       "        7.07595499, 12.36125381, 13.47578674,  7.53795304, 12.89012839,\n",
       "       11.23834278, 12.46618048, 15.69037895,  9.26383565,  7.73879151,\n",
       "        7.33145671,  7.55256915,  9.76869487, 11.02345592, 10.8300724 ,\n",
       "       12.48154975, 10.03900568, 14.36060223,  9.2060295 , 18.1605075 ,\n",
       "       11.87700204,  7.42852733,  6.78732251, 11.44741725,  9.32961164,\n",
       "       12.14200148, 11.41971287,  9.78151326,  7.45961885,  5.45545833,\n",
       "        8.66045514, 12.56919638, 10.64228123,  6.26278366, 10.51954278,\n",
       "       11.15595214,  7.34842769, 10.46117532, 10.17462616,  6.57108911,\n",
       "       11.07336208, 11.68235358, 13.24915373, 13.16140616,  5.8669919 ,\n",
       "        7.18652488, 11.5451058 , 11.54135785, 11.54514306, 21.55819447,\n",
       "       11.71267153, 13.40669692, 12.86200529, 11.95417375,  9.05419227,\n",
       "       12.27690766,  7.68152436,  9.28954418,  8.54390936, 10.24562242,\n",
       "       16.9439757 ,  4.39820442, 12.05878057,  5.16185239,  8.5842044 ,\n",
       "       13.26685179, 10.19284006,  6.76676567,  7.85408887, 12.03879325,\n",
       "        7.8089001 , 10.64937577, 10.13671552,  8.04519896, 16.43183227,\n",
       "       11.90175707,  3.92457224, 10.55936294,  8.01464061, 12.5573    ,\n",
       "        7.62243778,  9.65579068, 11.51496184, 12.59726558,  6.39911078,\n",
       "        8.99649629,  8.57516407,  8.0400123 , 15.29636272, 11.21494513,\n",
       "        6.21734814, 12.75358584, 16.36646859, 13.09739578,  5.4418901 ,\n",
       "        8.54729778, 13.80073345,  7.8769916 , 11.33145828, 12.32390216,\n",
       "        7.21920859,  9.82142393,  0.27619798,  6.92683708,  9.24229555,\n",
       "        6.25665045, 14.89723391,  5.70957587,  8.67986654, 10.39222173,\n",
       "       14.32381987,  5.69241355, 13.48949126, 10.03069918,  7.05547405,\n",
       "       11.38631042, 10.59717909,  8.19934937, 10.20940625,  8.84405921,\n",
       "       10.34055204, 11.98639202, 14.75805045,  6.2865535 , 16.39910012,\n",
       "        4.1437366 ,  9.54464471, 11.76495162, 10.8429756 ,  8.13190144,\n",
       "        9.37563325,  8.5209972 ,  8.23190573, 12.54880629, 11.07104646,\n",
       "        7.92127121, 12.69879963, 10.92189856, 12.43858636, 11.88888653,\n",
       "        7.51301497,  8.31945688, 12.24188082, 11.8311108 ,  9.93729522,\n",
       "       10.35198215, 13.83299469,  8.22528583, 11.64129214,  9.39342204,\n",
       "        9.34695639, 13.29633056, 12.47624905, 12.44052891, 13.91643642,\n",
       "       10.06301152, 12.04585891,  9.06919973, 10.97249906,  9.60957084,\n",
       "       10.29098789, 11.78547108,  7.54533795, 16.27716183,  6.98194786,\n",
       "        6.35743416, 13.47433262, 12.37498808, 11.87235945, 11.88503653,\n",
       "        9.96325968,  7.30823689, 10.22741367,  7.96851487, 12.9253592 ,\n",
       "        9.55882786,  7.52350841,  9.03584248, 11.23879436,  8.30882634,\n",
       "        7.53333881, 10.73106163, 10.73489971,  8.47917047,  8.58688508,\n",
       "       10.69614981,  5.65574698,  5.77760868,  7.84466734,  9.35965854,\n",
       "       10.9327227 , 14.42606865, 12.57297887,  9.52018441,  9.94295138,\n",
       "        6.99241191,  9.94446059,  9.13402408, 10.96815568,  7.51830717,\n",
       "       11.55803954, 14.59821674,  9.67371955, 11.20513517, 12.07043198,\n",
       "        8.79633858, 10.67227745, 10.0377772 , 10.2930283 ,  7.68097065,\n",
       "       10.07353052, 11.49399487, 14.35343082, 12.87781248, 16.45954737,\n",
       "        7.69795731, 12.61696191, 10.55002602, 16.5694088 ,  7.57510514,\n",
       "        7.48083447,  8.20182206,  3.62831283,  8.42273493,  7.72260202,\n",
       "       10.45118136, 11.02526793, 15.62851252, 12.85127151,  8.26928903,\n",
       "        7.30475599, 11.47575751,  6.03930038, 15.4943763 , 13.53832036,\n",
       "        8.59247304,  4.86059641, 14.06161712,  9.65638046, 13.71344894,\n",
       "        5.21671702,  8.20187493, 10.0157311 , 10.14094178,  8.64980359,\n",
       "       11.8685498 ,  6.79713871,  9.57286154, 10.3608869 , 11.5433165 ,\n",
       "       12.13484463,  6.62607372,  5.39765749, 13.83303047, 10.99694204,\n",
       "        7.75454039, 14.65345593, 10.3470239 , 13.53789155, 10.20255544,\n",
       "       16.18224377, 15.26602253,  9.25310755, 12.91471285, 11.93612785,\n",
       "       14.10589467,  7.10522962, 12.05815438, 13.17527346,  4.72378154,\n",
       "        6.45022446,  3.88230347,  9.1917795 , 12.15262677, 14.50707116,\n",
       "       10.22228434, 14.88584664,  5.85969563,  4.88985268,  9.8333569 ,\n",
       "       11.15219635,  9.90191576,  3.7976737 ,  9.73263988,  6.0865915 ,\n",
       "       12.00901765, 11.09979474,  7.18036064,  8.45839925,  6.82235943,\n",
       "        9.81196271, 12.86542696,  7.04282186, 11.51213955,  8.40922714,\n",
       "        7.6213815 ,  9.67890892,  6.89427303,  8.33905208,  6.40636632,\n",
       "       15.8941754 , 10.10579066,  7.90082348, 10.64193973,  9.66301585,\n",
       "        9.3370912 , 11.8425001 , 12.27252313,  8.40849656,  8.27254528,\n",
       "        9.17484491,  3.09423651,  5.45442681, 14.1006228 , 14.93490314,\n",
       "        9.25289188, 11.72967089, 10.93375046, 19.23664243, 13.35872473,\n",
       "        9.61624723,  7.13337868,  5.18066104, 10.61039091,  7.73094776,\n",
       "        5.73323887,  8.06028135,  6.75535599, 15.06142491, 12.64491927,\n",
       "        9.97608208, 14.43983242, 10.23210492,  7.4161474 , 14.56937223,\n",
       "       11.61673013,  6.88826154,  9.42898397,  7.37314524,  5.85160081,\n",
       "       12.77853264, 15.72824992,  5.80429728, 11.68890771,  8.04807229,\n",
       "        8.53862385,  8.22281823,  7.40802769, 10.14556488,  7.50714965,\n",
       "       10.81137048,  9.84928567,  9.28315586,  7.27730901,  8.26968601,\n",
       "       12.26617368, 11.50275156,  7.06733427, 10.29799692, 12.25416137,\n",
       "        4.99178416, 11.63008058,  8.01212872, 11.71179601,  7.71022253,\n",
       "        4.5853537 ,  5.11737269, 10.14425484, 10.77916751,  7.28705012,\n",
       "       11.91577738,  5.01543981,  9.8017606 ,  6.3669514 ,  8.04449168,\n",
       "       10.14219601,  7.4187599 ,  8.84633337, 13.01887843,  8.26932439,\n",
       "       12.50707634,  6.61087944, 11.58941253, 14.32470586,  2.5850665 ,\n",
       "        7.60931423, 11.73121638,  9.39086384, 11.11343762,  8.18804444,\n",
       "       10.25976936,  9.53296829, 13.50334618, 10.76326253, 11.01280799,\n",
       "        8.7643691 ,  8.53718133,  8.70232544, 11.18335643,  8.73704656,\n",
       "       10.86932457, 16.2262024 , 12.61337411,  9.0219294 , 13.60364177,\n",
       "        8.77577388,  3.88562639,  6.97574107,  4.38762424,  8.94545955,\n",
       "       10.05525514, 15.02931194, 10.98078212,  9.34269841, 12.48821674,\n",
       "        3.36659407, 10.70684367, 12.31259558,  5.56424126, 13.43126213,\n",
       "       11.01548922,  8.75413626, 11.8983456 , 16.81207857, 10.54559877,\n",
       "       10.74466176,  8.6219173 ,  7.45046689, 12.49100745,  7.43174852,\n",
       "       10.21469871,  8.56702766, 11.43693948, 11.00098632, 13.11261983,\n",
       "        8.4699508 ,  9.19037519,  7.06370885,  8.66712022, 11.13190148,\n",
       "       12.27096585,  7.23350403, 12.60881776, 14.06691358, 11.24030471,\n",
       "       15.63038744,  7.6786324 ,  6.26603589,  4.66383925, 14.48813293,\n",
       "       11.96309697,  9.83324599, 10.83990588,  6.62353286, 17.33725594,\n",
       "       10.38766355, 10.32818438, 12.17729987, 11.4430277 , 10.67165207,\n",
       "        7.62857663, 11.41440507, 15.64607349, 14.03626014, 14.77955988,\n",
       "        8.46635297,  7.03118554,  9.62263924, 10.16717474, 13.28257456,\n",
       "        4.92260611, 14.58865096,  9.5259763 ,  8.71935679,  6.96368687,\n",
       "        5.03542998, 12.46951175, 10.2199539 ,  6.1301173 ,  6.11476368,\n",
       "        8.9926459 , 15.00706458,  9.22122595,  5.49057114,  9.26277081,\n",
       "        9.18182929,  1.90934007,  9.8371154 ,  9.30719641, 12.08861909,\n",
       "       15.54686828, 13.37969509,  9.19333393,  6.68042227, 17.72007941,\n",
       "       10.1776553 , 10.04178788,  9.92762474, 10.59425428,  9.56691876,\n",
       "        8.27901398,  8.35942318,  9.90174019,  8.36972569,  7.86146265,\n",
       "       10.31929068,  9.23506835, 14.51197897,  2.04709057, 13.27452056,\n",
       "       13.73825558,  3.7798293 ,  8.97193722,  8.8856774 ,  5.77746492,\n",
       "        7.66654994,  6.66827246, 15.25681133, 12.80703518, 13.81466528,\n",
       "       12.16501619,  6.61284469,  8.4264392 , 11.46812368,  6.33361657,\n",
       "       12.13899529,  9.27902381,  8.87553758, 12.1328799 , 11.33278993,\n",
       "        8.9171015 , 13.47798941,  6.75681002, 11.84780682, 11.77930377,\n",
       "        9.07136068, 10.97839907,  6.24665927, 12.77208106,  9.44529359,\n",
       "        8.43183094, 13.14702768,  7.88696893,  5.77461611,  5.33011248,\n",
       "       11.81802985,  6.15871194, 15.26438255,  3.75421178, 15.0893691 ,\n",
       "       10.6330524 ,  9.70986066,  8.36524274, 11.19740834,  9.88709589,\n",
       "       13.30990565, 10.34268295, 10.45090528,  8.90916336,  9.82916313,\n",
       "       10.92340531,  4.86949482,  5.95544373, 12.22979228, 10.51259631,\n",
       "        9.44804999, 10.0553018 , 11.04274512,  8.38072096,  7.66508582,\n",
       "       10.58753577,  7.06488167, 11.22475827,  4.89224919, 13.08746691,\n",
       "       11.41779245, 10.7680892 , 12.94807295, 14.99642333, 13.0431102 ,\n",
       "        4.47737731,  6.1612691 ,  8.12554427, 10.07827315, 11.55297706,\n",
       "        7.82276856, 10.56030029,  7.7338512 ,  8.16544659,  5.78001671,\n",
       "        7.23030026,  5.94494618,  7.07238024, 13.16092539,  7.15180333,\n",
       "       17.89714619, 11.4799537 , 10.55450837,  7.42492666, 12.10092964,\n",
       "        8.27308652, 10.36602944, 17.68025361,  9.7118203 , 13.44781998,\n",
       "        7.89047072,  9.89503453, 15.31240191,  8.11909883, 15.43734567,\n",
       "       12.12325581,  8.31259967, 11.89722322, 12.91766335, 11.86542989,\n",
       "        5.28932584,  7.81858847,  9.25744409,  9.77669971, 11.86201629,\n",
       "       10.533103  ,  5.99396692, 11.14059355, 11.83175724, 11.67937134,\n",
       "       13.24234218, 12.50176646, 11.37754024,  9.78950287,  5.0171172 ,\n",
       "       11.28885466, 10.62306306, 10.81473651,  6.16975427,  6.75683038,\n",
       "       13.15945856,  9.88133454, 12.04450209, 10.08495513, 10.08926842,\n",
       "       12.81485142,  8.45186582, 10.28836233,  8.61317413,  8.69651132,\n",
       "        9.07248363, 10.66640131,  8.56375414, 13.76726838,  7.31617809,\n",
       "        9.43938507,  8.68080683, 14.34093365, 10.58966433, 13.09553362,\n",
       "        5.54331888, 10.8011508 , 12.66889239, 10.24685197, 13.19644113,\n",
       "        8.44813465, 14.22804232, 16.89669437,  8.91148432,  8.66349244,\n",
       "       14.36015343, 14.73871644,  8.43141992,  8.73943955,  9.15464617,\n",
       "        5.96664847,  7.24404416,  6.9875777 ,  7.6966073 ,  9.89594534,\n",
       "       10.7026442 , 14.65150148,  7.00493788, 12.9529672 ,  9.35803347,\n",
       "        9.85160887, 12.02445848,  6.63183394, 11.14722924, 10.49935662,\n",
       "       11.47735379, 10.86750593, 17.36590042,  8.08678005,  8.40700913,\n",
       "        8.13057842,  8.33356864,  8.08783862, 13.56704959, 14.26151274,\n",
       "        8.28776112,  7.50293328, 11.41424667,  8.34333087, 11.89879545,\n",
       "       10.60876906,  5.45276766, 14.6425156 , 15.38763302,  8.16163393,\n",
       "        8.83689532, 10.85759617, 11.00337037, 11.97563282, 16.03061362,\n",
       "        9.46915832,  7.60510827,  5.86204232,  7.80720988,  9.90061908,\n",
       "       15.38367359,  8.4471661 , 10.67136385,  9.95073131, 13.56517982,\n",
       "       17.58079728,  8.40739368,  8.53168167, 13.13248263, 12.04567447,\n",
       "       15.54012198, 11.75178456,  8.92212373, 11.77196449, 13.32611074,\n",
       "       12.46144654, 11.52182209, 13.20002407, 13.50788677, 14.14647697,\n",
       "       11.94612966,  9.49864576, 10.44014106, 13.6195269 ,  7.54919299,\n",
       "       11.10601993,  8.81998356, 10.08623447, 13.83535559, 10.5732972 ,\n",
       "       10.13930964,  5.92043158, 12.2387607 , 11.93645254, 16.48976417,\n",
       "        9.0766653 , 10.65745098, 10.74815105, 14.73235984,  9.7141134 ,\n",
       "       10.83706458, 11.82368953, 10.55982737,  8.66069916, 10.58226998,\n",
       "       13.22089525,  6.9204541 , 10.39890902,  7.89963756, 13.58513989,\n",
       "        5.43043929,  8.32323446, 11.13163563, 14.69657209,  9.80274922,\n",
       "        8.33440142, 15.64347121,  5.6559583 ,  3.40358213, 11.32004335,\n",
       "        8.49383733,  6.93630155, 12.12506934, 10.73140214,  8.30776411,\n",
       "        6.1590868 , 12.61737198, 11.95060353,  9.70247241, 15.53991099,\n",
       "        6.7897457 ,  5.42342449,  7.92427579,  9.86324195, 10.73001835,\n",
       "        9.27629183, 11.05616619,  6.24538173, 14.33129381,  9.75354646,\n",
       "       13.35188749, 11.02817604, 11.37025966, 11.70930184, 11.34312568,\n",
       "       11.92816828, 13.98745759, 10.58956351, 12.12701127,  9.73079292,\n",
       "       14.32035165,  7.97082309, 15.4028213 ,  9.87952615,  5.70767469,\n",
       "       10.38431324,  7.95684503, 12.52193065,  8.04212806,  8.6614497 ,\n",
       "        4.33137781,  8.64308104,  2.72836202,  5.24829153, 12.28124397,\n",
       "       12.35740048, 11.27637269,  7.09907157,  9.85686593,  9.98919238,\n",
       "        6.52490593, 14.51019491, 12.63208687,  9.33710748, 10.08065752,\n",
       "       10.62514842,  3.87479539,  9.25846785,  7.95404726,  6.99513997,\n",
       "        9.15669912, 15.39305958, 11.92252858,  8.28646303, 11.71774834])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norm(loc=10, scale=3).rvs(size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from scipy.stats._distn_infrastructure import rv_continuous\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class SelectAboveNoise(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # não é sklearn API friendly, mas acho útil aqui\n",
    "    def __init__(self, sampler:Type[rv_continuous], base_estimator=None, random_state=None):\n",
    "        self.sampler = sampler\n",
    "        self.base_estimator = base_estimator\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _validate_estimator(self, default):\n",
    "        if self.base_estimator is not None:\n",
    "            self.base_estimator_ = self.base_estimator\n",
    "        else:\n",
    "            self.base_estimator_ = default\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = self._validate_data(\n",
    "            X,\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "        \n",
    "        self._validate_estimator(RandomForestClassifier(random_state=self.random_state))\n",
    "        \n",
    "        self.base_estimator_.fit(X, y)\n",
    "        \n",
    "        self._selected_index_list_ = \\\n",
    "        (pd.DataFrame(list(zip(range(self.n_features_in_), self.base_estimator_.feature_importances_)),\n",
    "                      columns=['feature_name', 'feature_importance'])\n",
    "         .sort_values(by='feature_importance', ascending=False)\n",
    "         .head(self.K)\n",
    "         .index\n",
    "         .to_list()\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "        return np.array([feat in self._selected_index_list_ for feat in range(self.n_features_in_)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog_datalab_boruta2",
   "language": "python",
   "name": "blog_datalab_boruta2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
